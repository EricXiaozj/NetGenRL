{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "from cgan_code_2 import Discriminator  # 假设你有一个定义好的 Discriminator 类\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import struct\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_LEN = 114\n",
    "# NPRINT_REAL_WIDTH = 50*8\n",
    "NPRINT_REAL_WIDTH = 22*8\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'aim': 2, 'email': 3, 'voipbuster': 4, 'hangouts': 5, 'youtube': 6, 'sftp': 7, 'icq': 8,  'ftps': 9, 'vimeo': 10, 'spotify': 11, 'netflix': 12, 'bittorrent': 13}\n",
    "LABEL_DICT = {'facebook': 0, 'skype': 1}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'hangouts': 4, 'youtube': 5, 'ftps': 6, 'vimeo': 7, 'spotify': 8, 'netflix': 9, 'bittorrent': 10}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'youtube': 4, 'ftps': 5, 'vimeo': 6, 'spotify': 7, 'netflix': 8, 'bittorrent': 9}\n",
    "# LABEL_DICT = {'email': 0, 'youtube': 1, 'ftps': 2, 'vimeo': 3, 'spotify': 4, 'netflix': 5, 'bittorrent': 6}\n",
    "\n",
    "SEQ_DIM = 3\n",
    "MAX_PKT_LEN = 1500\n",
    "MAX_TIME = 1000\n",
    "MAX_PORT = 65536\n",
    "MAX_SEQ_LEN = 16\n",
    "\n",
    "label_dim = len(LABEL_DICT) \n",
    "image_dim = (1, NPRINT_REAL_WIDTH, NPRINT_REAL_WIDTH)  # 生成单通道图像\n",
    "noise_dim = 128  # 噪声维度\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "source_name = './vpn_data_small.json'\n",
    "bins_name = './bins_small.json'\n",
    "model_name = './save_all/discriminator_v7.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_data = {}\n",
    "with open(bins_name, 'r') as f_bin:\n",
    "    bins_data = json.load(f_bin)\n",
    "    \n",
    "port_intervals = bins_data['port']['intervals']\n",
    "pkt_len_intervals = []\n",
    "for bins in bins_data['packet_len']:\n",
    "    pkt_len_intervals.append(bins['intervals'])\n",
    "time_intervals = []\n",
    "for bins in bins_data['time']:\n",
    "    time_intervals.append(bins['intervals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292256/1666204638.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_name)  # 加载保存的权重字典\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (lstm): LSTM(3, 512, num_layers=4, batch_first=True)\n",
       "  (length_fc): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (heads): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(label_dim, SEQ_DIM, MAX_SEQ_LEN, 'cpu')\n",
    "\n",
    "# 加载模型权重\n",
    "checkpoint = torch.load(model_name)  # 加载保存的权重字典\n",
    "discriminator.load_state_dict(checkpoint)  # 将权重字典加载到模型中\n",
    "\n",
    "# 切换到评估模式\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lstm.weight_ih_l0',\n",
       "              tensor([[-0.0573, -0.1975, -0.0593],\n",
       "                      [-0.1327, -0.1763, -0.2676],\n",
       "                      [ 0.0375, -0.0923,  0.1357],\n",
       "                      ...,\n",
       "                      [ 0.0051,  0.1225, -0.1281],\n",
       "                      [-0.1791,  0.0186,  0.0578],\n",
       "                      [-0.1543, -0.0544, -0.0151]], device='cuda:0')),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.2117,  0.1721, -0.1151,  ...,  0.0525, -0.0521, -0.2237],\n",
       "                      [ 0.0655, -0.0545,  0.0064,  ...,  0.1071,  0.0044,  0.0445],\n",
       "                      [ 0.0699, -0.0598, -0.0008,  ..., -0.0155,  0.1059,  0.0137],\n",
       "                      ...,\n",
       "                      [-0.0335, -0.0617, -0.0134,  ..., -0.0827,  0.0627, -0.0322],\n",
       "                      [-0.0976,  0.0960, -0.2138,  ...,  0.1979, -0.1245, -0.2666],\n",
       "                      [ 0.0127,  0.0096,  0.0347,  ..., -0.0569,  0.0229,  0.0709]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 0.0828,  0.0777,  0.0493,  ...,  0.0220,  0.2068, -0.0085],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.1110,  0.0783,  0.0577,  ...,  0.0080,  0.2413, -0.0071],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_ih_l1',\n",
       "              tensor([[-0.0780, -0.0499, -0.0234,  ...,  0.0373, -0.1510,  0.0075],\n",
       "                      [ 0.0124,  0.0175, -0.1005,  ...,  0.0435, -0.0732, -0.0174],\n",
       "                      [ 0.2560, -0.0543, -0.1176,  ..., -0.0267, -0.0297,  0.0024],\n",
       "                      ...,\n",
       "                      [-0.0234, -0.1270,  0.0150,  ..., -0.0543, -0.0167,  0.0266],\n",
       "                      [-0.0339, -0.0628, -0.0513,  ...,  0.1038, -0.0645, -0.1007],\n",
       "                      [ 0.0589, -0.0960, -0.0209,  ..., -0.0352,  0.1105,  0.0364]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_hh_l1',\n",
       "              tensor([[-0.0848, -0.0085,  0.0098,  ...,  0.0593, -0.0732, -0.0722],\n",
       "                      [-0.0163,  0.0653,  0.0520,  ..., -0.0515, -0.0724, -0.0032],\n",
       "                      [-0.1314,  0.1384,  0.0356,  ..., -0.0026,  0.0859,  0.0737],\n",
       "                      ...,\n",
       "                      [ 0.0480,  0.0381,  0.0504,  ...,  0.0102, -0.0075,  0.0522],\n",
       "                      [ 0.0358, -0.0043, -0.0439,  ...,  0.0039,  0.0278,  0.0506],\n",
       "                      [ 0.0107, -0.0759,  0.0462,  ...,  0.0385, -0.0505, -0.0407]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l1',\n",
       "              tensor([0.0904, 0.0612, 0.1238,  ..., 0.0498, 0.0346, 0.0550], device='cuda:0')),\n",
       "             ('lstm.bias_hh_l1',\n",
       "              tensor([ 0.0427,  0.0374,  0.1524,  ..., -0.0016,  0.0900,  0.0515],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_ih_l2',\n",
       "              tensor([[ 0.0261,  0.0141, -0.0503,  ..., -0.0452,  0.0959,  0.0684],\n",
       "                      [-0.1071,  0.0158,  0.0442,  ..., -0.0330, -0.0559, -0.0517],\n",
       "                      [ 0.0456, -0.0894, -0.0570,  ..., -0.0592,  0.0023,  0.0187],\n",
       "                      ...,\n",
       "                      [ 0.0256, -0.0744, -0.1659,  ..., -0.0366, -0.0104, -0.0795],\n",
       "                      [ 0.0184, -0.0209,  0.0546,  ..., -0.0062, -0.0400, -0.0064],\n",
       "                      [ 0.0425,  0.0148, -0.0582,  ...,  0.0528, -0.1170, -0.0081]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_hh_l2',\n",
       "              tensor([[ 0.0577, -0.0237,  0.0332,  ..., -0.0297, -0.0183,  0.0228],\n",
       "                      [-0.0153,  0.0005, -0.0329,  ...,  0.0318,  0.0997, -0.0844],\n",
       "                      [ 0.0416, -0.0797, -0.0105,  ...,  0.0028, -0.0221,  0.0429],\n",
       "                      ...,\n",
       "                      [ 0.0714,  0.0131,  0.0591,  ...,  0.0475,  0.0117, -0.0509],\n",
       "                      [ 0.0066,  0.0300,  0.0552,  ..., -0.0071,  0.0144,  0.0385],\n",
       "                      [ 0.0248,  0.0412,  0.0431,  ..., -0.0029, -0.0348,  0.0367]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l2',\n",
       "              tensor([0.0213, 0.0233, 0.0038,  ..., 0.0480, 0.0142, 0.0258], device='cuda:0')),\n",
       "             ('lstm.bias_hh_l2',\n",
       "              tensor([ 0.0010,  0.0397, -0.0370,  ..., -0.0067,  0.0306, -0.0164],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_ih_l3',\n",
       "              tensor([[-2.2242e-02,  6.4434e-02, -3.1997e-02,  ...,  1.1476e-02,\n",
       "                        1.1648e-02, -6.9260e-02],\n",
       "                      [ 3.9729e-02, -4.1522e-02,  5.5800e-02,  ..., -9.1949e-03,\n",
       "                       -2.8258e-02,  1.9836e-02],\n",
       "                      [ 1.0567e-01, -5.5320e-03, -1.6320e-03,  ...,  3.0618e-02,\n",
       "                        8.3235e-02, -3.2599e-02],\n",
       "                      ...,\n",
       "                      [ 1.8253e-02,  1.7284e-03,  2.9284e-02,  ...,  6.7237e-02,\n",
       "                        3.8353e-02, -1.0141e-02],\n",
       "                      [ 1.6764e-02,  4.4908e-02,  7.5913e-03,  ..., -1.1170e-02,\n",
       "                       -2.5971e-02, -2.5597e-03],\n",
       "                      [-1.3654e-02,  2.2864e-02, -5.7798e-05,  ...,  2.3201e-02,\n",
       "                       -2.9622e-04, -1.8004e-02]], device='cuda:0')),\n",
       "             ('lstm.weight_hh_l3',\n",
       "              tensor([[-0.0028,  0.0331,  0.0022,  ..., -0.0199,  0.0146,  0.0279],\n",
       "                      [ 0.0023,  0.0131,  0.0265,  ...,  0.0149, -0.1213,  0.0557],\n",
       "                      [-0.0026, -0.0655,  0.0092,  ..., -0.0068, -0.0110, -0.0825],\n",
       "                      ...,\n",
       "                      [ 0.0237,  0.0705,  0.0435,  ...,  0.0090,  0.0253, -0.0558],\n",
       "                      [ 0.0544, -0.0091, -0.0731,  ..., -0.0869,  0.0073,  0.0032],\n",
       "                      [ 0.0016,  0.0362,  0.0395,  ..., -0.0550, -0.0171, -0.0779]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l3',\n",
       "              tensor([ 0.0162,  0.0390, -0.0218,  ...,  0.0412,  0.0103,  0.0159],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_hh_l3',\n",
       "              tensor([-0.0157,  0.0121,  0.0231,  ..., -0.0088, -0.0292, -0.0028],\n",
       "                     device='cuda:0')),\n",
       "             ('length_fc.0.weight',\n",
       "              tensor([[-3.5899e-01,  1.2841e-03, -5.5830e-02,  ...,  2.6666e-01,\n",
       "                        4.3414e-01,  3.3147e-01],\n",
       "                      [ 1.6483e-01,  1.8562e-01,  6.7513e-02,  ...,  1.8028e-01,\n",
       "                       -2.2047e-01,  2.4959e-01],\n",
       "                      [-9.9018e-02,  1.6832e-01, -3.0605e-01,  ..., -5.3361e-02,\n",
       "                        4.0727e-01,  2.8332e-01],\n",
       "                      ...,\n",
       "                      [-6.2006e-02, -1.4851e-01,  8.6047e-02,  ..., -2.4784e-01,\n",
       "                        6.9752e-01,  5.3540e-01],\n",
       "                      [-2.0014e-01, -1.7246e-01, -2.3210e-01,  ...,  1.0151e-01,\n",
       "                       -4.8048e-04,  4.0860e-01],\n",
       "                      [-8.2671e-02, -2.9237e-02,  3.3099e-01,  ...,  4.9399e-01,\n",
       "                        5.6620e-01,  9.0868e-02]], device='cuda:0')),\n",
       "             ('length_fc.0.bias',\n",
       "              tensor([ 0.3564,  0.1202,  0.2830,  0.0004,  0.0922, -0.0053,  0.1411,  0.0815,\n",
       "                      -0.2353, -0.1095, -0.1352, -0.2162,  0.1496, -0.0469,  0.1578, -0.0037,\n",
       "                       0.0619,  0.1209, -0.0596,  0.0859, -0.2130,  0.0852, -0.0110,  0.2746,\n",
       "                      -0.0053,  0.2113,  0.0265,  0.2327,  0.1489,  0.2891,  0.2748,  0.2983,\n",
       "                       0.3375,  0.2526,  0.1549, -0.2471,  0.2353,  0.1923,  0.2506, -0.0700,\n",
       "                       0.1969,  0.2298,  0.2419, -0.0191, -0.0210,  0.1200,  0.1956,  0.2838,\n",
       "                      -0.2490, -0.0575,  0.2214, -0.0082,  0.3372,  0.1852,  0.2619,  0.1023,\n",
       "                       0.2765,  0.2564, -0.0184,  0.1005,  0.1484,  0.0928,  0.1573,  0.2462,\n",
       "                       0.1873,  0.1931, -0.0893, -0.0292, -0.0276,  0.1345, -0.1066,  0.0720,\n",
       "                       0.2670,  0.0600,  0.1467,  0.2069,  0.1684,  0.1903, -0.0656,  0.2794,\n",
       "                      -0.0319, -0.2387,  0.1961,  0.0362,  0.2638,  0.0730,  0.1473,  0.2570,\n",
       "                       0.0828,  0.1271,  0.2165,  0.1889,  0.2811,  0.1488,  0.0738,  0.2154,\n",
       "                       0.2789, -0.0094, -0.0086, -0.0768,  0.2296, -0.1155,  0.2817, -0.1112,\n",
       "                       0.2286,  0.2071,  0.1696,  0.0497,  0.0150, -0.0846,  0.2328,  0.1472,\n",
       "                       0.2006,  0.1361, -0.0136,  0.0243,  0.0009,  0.2155,  0.1443,  0.0968,\n",
       "                       0.3312,  0.2426,  0.0857,  0.3174, -0.0775,  0.1821,  0.1983,  0.1875],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[ 0.0278,  0.0560,  0.0932,  ...,  0.0191, -0.0473, -0.0553],\n",
       "                      [ 0.0860,  0.0199,  0.0968,  ..., -0.0084,  0.0726, -0.0172],\n",
       "                      [-0.0207, -0.0050, -0.0013,  ...,  0.0220, -0.0051, -0.0220],\n",
       "                      ...,\n",
       "                      [ 0.0252,  0.0049, -0.0106,  ..., -0.0347,  0.0241, -0.0379],\n",
       "                      [ 0.0854,  0.0866,  0.0567,  ..., -0.0130, -0.1172, -0.0111],\n",
       "                      [ 0.1514,  0.1315,  0.1015,  ..., -0.0316,  0.0570,  0.0025]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.0.bias',\n",
       "              tensor([ 8.3134e-02,  7.9528e-02, -1.3436e-02,  2.5762e-02,  4.2408e-02,\n",
       "                       3.0709e-02,  8.0630e-02,  8.3993e-02,  1.4200e-02,  6.4353e-02,\n",
       "                       4.0609e-02,  2.7259e-02, -1.6991e-02,  6.6667e-02,  4.5123e-03,\n",
       "                       2.2611e-02,  4.7146e-02,  6.2970e-02,  8.4637e-02,  3.5241e-02,\n",
       "                       1.2448e-02,  5.1669e-02,  9.0000e-02,  1.8704e-02,  5.1954e-02,\n",
       "                       3.3425e-02,  6.4843e-02,  6.8775e-02,  2.7049e-02,  1.0093e-02,\n",
       "                       8.5288e-02,  9.3257e-02,  6.9718e-02,  3.1362e-02,  4.7380e-02,\n",
       "                       9.7602e-03,  7.6383e-02,  2.6372e-02,  3.7818e-02,  3.6349e-02,\n",
       "                       7.8213e-02,  4.1017e-02,  9.8151e-02, -6.4691e-02,  9.9201e-03,\n",
       "                       4.5179e-02,  1.0214e-02,  5.2383e-02,  6.1401e-02,  2.4793e-03,\n",
       "                      -6.9981e-03,  9.5601e-02,  5.2397e-02,  2.4969e-02,  4.2999e-02,\n",
       "                       1.5585e-01,  1.0683e-01,  1.4202e-02,  6.6888e-03,  6.6173e-02,\n",
       "                       1.8218e-01, -1.5780e-02,  4.3641e-02,  5.9560e-02,  7.0597e-02,\n",
       "                      -1.5527e-02,  9.5743e-02, -5.1412e-03,  6.1683e-02,  3.8556e-02,\n",
       "                       1.1766e-01,  2.7807e-02,  5.4640e-02,  2.8471e-02, -1.0556e-02,\n",
       "                       2.7702e-02,  5.9343e-02,  4.1992e-02,  6.1229e-02,  7.0864e-02,\n",
       "                       1.5054e-02,  4.5504e-02,  4.8420e-02,  8.3340e-02,  4.4976e-02,\n",
       "                       4.1629e-02,  6.2178e-02,  6.1811e-02,  2.1625e-02,  6.7310e-03,\n",
       "                       3.6113e-02, -1.0053e-01,  2.5108e-02,  1.1918e-01,  2.5681e-02,\n",
       "                       3.8276e-02,  5.6364e-03,  7.7810e-02,  1.6717e-02,  5.0139e-02,\n",
       "                       2.3687e-02,  7.8679e-02,  2.9386e-02,  9.1793e-02, -5.6595e-02,\n",
       "                      -6.0046e-02,  3.5875e-02,  1.5415e-04,  4.0000e-02,  7.4151e-02,\n",
       "                       1.4301e-01,  4.9062e-02,  7.0507e-02,  5.0634e-02,  4.4736e-02,\n",
       "                       3.5927e-02,  7.6401e-02,  7.0806e-02,  1.2129e-01,  4.0551e-02,\n",
       "                       2.3436e-04,  2.1572e-02,  8.0345e-02,  2.8994e-02,  2.7561e-02,\n",
       "                       1.8960e-02,  3.8073e-02,  6.2397e-02,  1.9338e-01,  8.5293e-03,\n",
       "                       5.6847e-02,  5.3242e-02, -4.5757e-02,  5.7962e-02,  5.4308e-02,\n",
       "                       4.4660e-02,  4.1924e-02,  5.1011e-02,  1.0748e-01,  2.0433e-02,\n",
       "                       6.1823e-02, -2.3344e-03,  9.5374e-02,  1.9198e-02,  7.7694e-03,\n",
       "                       2.3384e-02,  5.0900e-03,  4.5856e-02,  4.4104e-03,  4.4316e-02,\n",
       "                       5.6664e-02,  3.8730e-02, -2.7499e-03,  8.1954e-03,  3.2176e-02,\n",
       "                       4.2276e-02,  8.1656e-03,  3.6768e-02,  5.6660e-03,  2.5435e-02,\n",
       "                       3.7571e-02,  2.2928e-01,  6.2640e-02,  2.7162e-02,  5.9294e-02,\n",
       "                      -1.7386e-02,  1.0541e-01, -6.2029e-02,  6.0231e-02,  1.9564e-01,\n",
       "                       1.0435e-01,  1.1618e-01,  5.7560e-02,  6.6803e-02,  1.6991e-01,\n",
       "                       6.3072e-02,  4.4796e-02, -1.5058e-02,  7.4376e-02, -3.3728e-02,\n",
       "                       8.9940e-03,  5.8789e-02,  2.7939e-02,  2.4534e-01,  5.4738e-02,\n",
       "                       5.5255e-02,  3.9546e-02,  5.6968e-03,  7.2458e-02,  1.4747e-02,\n",
       "                       3.4909e-02, -5.3060e-03,  3.6986e-02,  8.6109e-02,  8.9995e-03,\n",
       "                       4.2816e-02,  7.1169e-02,  1.8334e-02,  4.2743e-02,  4.6629e-03,\n",
       "                       8.2795e-02,  3.2602e-02,  8.6656e-02,  6.7662e-02,  2.6689e-03,\n",
       "                      -1.7003e-02, -6.4881e-03,  5.2443e-02,  8.6128e-03, -4.1329e-03,\n",
       "                       8.1012e-03,  1.2828e-02, -1.6086e-02,  4.5953e-02,  2.2431e-02,\n",
       "                       6.2125e-02, -2.7566e-02,  4.5910e-02,  2.5599e-02,  1.7230e-02,\n",
       "                       5.3510e-02, -1.7921e-02, -2.7001e-02,  1.5715e-02, -2.7958e-02,\n",
       "                       6.6172e-02,  7.2807e-02,  4.8917e-02,  3.5715e-02,  9.0900e-02,\n",
       "                       5.8063e-02,  4.1319e-02,  6.2173e-02,  1.3415e-02,  5.2270e-02,\n",
       "                      -6.3583e-02,  9.7251e-02,  6.3547e-02,  7.4261e-03,  3.5465e-02,\n",
       "                       4.6726e-02,  6.5955e-02,  4.1709e-02,  4.7603e-02, -2.3617e-02,\n",
       "                       5.9718e-02,  2.8272e-02,  1.5167e-01, -7.0081e-02,  1.6440e-02,\n",
       "                      -3.7121e-02, -8.7347e-03, -5.3851e-03, -2.8656e-02,  4.5500e-02,\n",
       "                      -2.0624e-02,  5.8637e-02, -1.7809e-02,  7.2302e-02,  1.3646e-02,\n",
       "                       1.4521e-01,  6.3140e-02, -5.0683e-02,  1.0428e-01,  5.1078e-02,\n",
       "                       4.1600e-02, -6.7236e-03,  1.2844e-01,  4.2458e-03,  3.2200e-02,\n",
       "                       4.2396e-02,  7.6484e-02,  7.0307e-04,  9.1260e-02,  3.5946e-02,\n",
       "                       5.2403e-02,  1.6044e-02,  2.6662e-02,  3.8539e-02,  9.1322e-03,\n",
       "                       2.1721e-01,  2.4585e-02,  2.3769e-01,  8.6435e-02,  7.4657e-02,\n",
       "                       2.0725e-02,  6.9879e-02,  4.0904e-02,  4.2964e-02,  2.1784e-02,\n",
       "                       1.3058e-02, -2.4178e-02,  1.1278e-01,  3.9394e-02,  1.8019e-02,\n",
       "                       1.6984e-02,  6.4161e-02,  7.4558e-03,  1.1045e-01,  8.4699e-02,\n",
       "                      -4.4384e-02,  3.9732e-02,  1.6547e-02, -6.0682e-03,  8.3803e-03,\n",
       "                       9.1971e-02,  1.5910e-01,  4.0933e-02,  2.5798e-02,  1.3861e-02,\n",
       "                       5.6172e-02,  5.1020e-02,  1.2665e-01,  1.5772e-01,  7.0205e-02,\n",
       "                       2.4377e-02,  7.9365e-02,  3.2282e-02,  1.7996e-01,  8.5955e-03,\n",
       "                       1.6012e-02,  2.5286e-02,  1.0317e-02,  4.0335e-02,  3.9841e-02,\n",
       "                       5.6812e-02,  3.6478e-02,  4.6958e-02,  2.8887e-02,  2.7484e-02,\n",
       "                       3.9187e-02,  6.6682e-02, -1.0172e-02,  1.3576e-01,  3.6430e-02,\n",
       "                       6.1632e-02,  3.6792e-02,  7.1116e-02,  2.5390e-02, -4.6629e-02,\n",
       "                       3.3458e-02,  5.5594e-02,  5.9373e-02,  1.2008e-01,  8.1462e-02,\n",
       "                       2.2246e-01,  6.7874e-03,  9.2516e-02, -3.2079e-02,  1.4603e-01,\n",
       "                       6.9056e-02,  5.5941e-02,  7.5431e-05,  2.3197e-02,  3.1927e-02,\n",
       "                       1.7678e-02,  6.8886e-02,  6.2584e-02,  3.2215e-02,  7.4398e-03,\n",
       "                       8.0905e-02,  8.4619e-02,  7.7361e-02,  1.3293e-01,  8.4279e-02,\n",
       "                       4.4668e-03,  7.3552e-02, -1.2702e-02,  5.1191e-03,  5.8412e-02,\n",
       "                       4.6895e-02,  9.6966e-02,  5.6761e-02, -3.8219e-02,  6.2983e-02,\n",
       "                       5.9877e-02,  4.9619e-02,  1.4363e-02, -2.1006e-02,  1.1055e-01,\n",
       "                       3.6544e-02,  2.5667e-02,  7.8043e-02,  4.3340e-02,  3.1540e-03,\n",
       "                       2.2511e-02,  8.3109e-02,  6.6414e-02,  4.8174e-02,  3.1611e-02,\n",
       "                       1.3963e-02,  9.1423e-02,  4.7649e-02,  9.6398e-03,  4.5268e-02,\n",
       "                       8.9748e-02,  8.7679e-02,  4.5670e-03,  1.8369e-02,  4.8533e-03,\n",
       "                       6.8536e-02,  8.7963e-02,  2.8285e-02,  5.4732e-02,  1.0395e-01,\n",
       "                       9.9793e-02,  1.0283e-01, -5.4359e-03,  6.7745e-03, -9.6711e-03,\n",
       "                       2.4763e-02,  6.6488e-02, -4.6812e-04,  8.4855e-02,  4.0139e-02,\n",
       "                      -3.1811e-02,  8.7334e-02,  1.1718e-01,  8.3120e-02,  1.7964e-02,\n",
       "                       2.5444e-02,  1.0392e-02,  3.6661e-02,  6.1591e-04,  4.7744e-02,\n",
       "                       3.3763e-03,  4.8550e-02,  7.2352e-02,  6.7110e-03,  8.0805e-02,\n",
       "                       8.1463e-02,  8.0727e-02,  7.2393e-02,  1.4740e-01, -4.8111e-04,\n",
       "                      -2.3788e-02,  1.4468e-02,  7.3302e-02,  3.3983e-03,  2.2605e-02,\n",
       "                       5.3665e-02,  1.7387e-02,  7.7711e-02,  3.9702e-02,  2.6805e-02,\n",
       "                      -2.7544e-03,  2.3032e-02,  2.3432e-02,  4.0005e-02,  2.2222e-02,\n",
       "                       4.2496e-02,  1.5443e-02, -1.5859e-02,  1.5445e-02,  5.4117e-02,\n",
       "                       6.8227e-02,  3.0574e-02,  3.8313e-02,  1.0487e-02,  5.4737e-02,\n",
       "                       4.8262e-02,  2.1723e-02,  3.2871e-02,  1.4727e-02,  4.4591e-02,\n",
       "                       3.0774e-02,  3.5058e-02,  2.0007e-02, -1.2612e-03,  5.3340e-02,\n",
       "                       2.0585e-02,  5.0366e-02,  9.1128e-02,  7.2063e-02,  3.6779e-02,\n",
       "                       1.4638e-02,  1.6479e-01,  2.7655e-02,  7.3092e-02, -3.5499e-02,\n",
       "                      -5.2803e-02,  9.4756e-02,  8.7864e-02,  9.3580e-02,  5.3449e-02,\n",
       "                       6.6274e-02,  3.2654e-02,  5.4366e-02,  7.5085e-02,  7.3637e-02,\n",
       "                       2.9252e-02,  6.9266e-02,  5.9202e-02,  1.0690e-01,  3.4485e-02,\n",
       "                       2.9383e-02,  6.6188e-02,  3.4412e-02,  6.2021e-02,  7.8087e-02,\n",
       "                       3.6905e-02,  9.1986e-02,  2.6195e-02,  1.0349e-01, -4.4396e-02,\n",
       "                       6.7121e-02,  4.5507e-02,  7.7102e-03,  5.8535e-02,  2.9219e-02,\n",
       "                       5.1212e-02,  1.0640e-01], device='cuda:0')),\n",
       "             ('heads.0.0.weight',\n",
       "              tensor([[-3.9341e-03, -2.0157e-02,  2.9795e-02,  6.3849e-03, -3.8355e-03,\n",
       "                       -2.0755e-03, -1.7829e-03, -1.0373e-03, -2.5277e-03,  1.0812e-03,\n",
       "                       -3.5413e-03,  4.5265e-05, -1.6146e-03,  3.5063e-02, -2.4344e-02,\n",
       "                        4.8162e-03,  9.8774e-04, -1.0803e-03, -1.2435e-03,  2.0154e-02,\n",
       "                       -1.3946e-03,  1.6577e-04, -1.1920e-03,  1.5420e-02,  8.9624e-04,\n",
       "                        7.7744e-04,  1.6059e-03, -5.3133e-03,  2.8033e-03,  1.9876e-03,\n",
       "                        1.3627e-03, -8.1902e-03, -2.7461e-03,  3.5114e-03,  1.8573e-03,\n",
       "                        4.6453e-02,  1.1470e-03,  3.3559e-03,  2.0428e-03,  2.0824e-03,\n",
       "                        3.8944e-04,  7.6570e-04,  3.6598e-04,  1.5436e-02, -3.3430e-03,\n",
       "                       -2.0179e-03, -1.0408e-02,  3.0890e-04,  1.5179e-03,  2.8271e-03,\n",
       "                        1.5705e-03,  1.4605e-02, -1.7382e-04,  9.9654e-04, -4.9174e-03,\n",
       "                        4.9761e-05,  1.2342e-03,  1.6126e-03,  1.3779e-03,  2.5789e-03,\n",
       "                        5.3231e-04, -4.4188e-03,  3.7584e-03,  1.7666e-02,  2.8681e-03,\n",
       "                        2.4639e-03,  2.8633e-03,  7.5671e-04,  1.5142e-03, -2.6493e-03,\n",
       "                        2.5488e-04,  4.5619e-03,  2.6061e-03,  3.7006e-04,  4.4168e-02,\n",
       "                       -1.0992e-03,  2.7825e-03, -8.0651e-04,  1.9744e-03,  1.1225e-02,\n",
       "                       -7.8558e-04,  2.7219e-03, -1.5438e-04, -1.1562e-03,  1.6357e-03,\n",
       "                        3.5060e-04,  8.2857e-04, -3.1207e-03,  4.0967e-04,  2.0062e-03,\n",
       "                       -2.2249e-03, -2.6168e-03, -1.6866e-03, -4.7386e-04, -1.3015e-03,\n",
       "                        1.7763e-02,  2.7099e-03, -3.5785e-06, -1.5467e-01,  2.3060e-03,\n",
       "                       -9.9021e-04,  4.1799e-05,  4.1409e-03, -3.7524e-03,  4.9054e-02,\n",
       "                       -3.7148e-02,  4.1046e-03,  9.4370e-04,  2.4568e-02, -6.1035e-05,\n",
       "                       -2.1712e-02,  2.6516e-04,  1.6400e-04, -7.6104e-05,  1.0966e-03,\n",
       "                        1.4471e-02,  3.0263e-03, -2.0821e-03,  2.2603e-03, -1.8613e-03,\n",
       "                        3.0938e-02,  8.4886e-03,  2.9625e-03, -2.5603e-05, -1.2111e-03,\n",
       "                        1.2694e-03,  1.2562e-03,  4.2333e-02,  5.5969e-03,  2.1650e-02,\n",
       "                        1.0670e-03,  1.3233e-03,  6.1009e-02,  4.8911e-03,  4.4457e-04,\n",
       "                       -7.0148e-04,  1.5214e-03,  7.3399e-03,  1.1763e-03, -1.8957e-03,\n",
       "                       -1.4218e-03, -1.3085e-02,  3.3508e-03, -6.1775e-02,  1.0489e-03,\n",
       "                       -2.3466e-03,  3.4731e-03,  8.0986e-03,  1.3671e-03, -1.5208e-03,\n",
       "                       -3.4592e-04, -1.1104e-03,  4.5711e-03,  1.9845e-03,  7.0157e-04,\n",
       "                       -5.9221e-04,  1.2337e-03, -1.7610e-03,  1.1629e-03, -2.4662e-03,\n",
       "                        2.0898e-03, -3.4616e-02, -1.1699e-03,  1.4978e-03, -2.3818e-03,\n",
       "                        4.8771e-03, -1.7594e-03,  3.5013e-03,  3.4204e-04, -5.9508e-02,\n",
       "                       -4.1920e-03,  6.7361e-04, -3.3718e-03,  3.8519e-04, -6.3101e-02,\n",
       "                        1.5644e-03, -2.4014e-03, -1.5326e-03,  5.5443e-04,  5.4566e-02,\n",
       "                       -3.0716e-04,  1.3910e-03,  3.3778e-02,  1.7330e-03,  1.5976e-02,\n",
       "                       -1.1458e-03,  1.6574e-03,  5.6936e-05,  2.1956e-04,  1.2490e-03,\n",
       "                       -1.2222e-03,  5.3600e-03,  1.1868e-03,  1.6452e-03,  9.2180e-04,\n",
       "                       -2.4682e-03,  9.2060e-04,  8.8453e-04,  5.5267e-04,  7.3859e-02,\n",
       "                        3.0807e-03, -1.1021e-04,  2.4434e-03, -8.3751e-04, -2.5410e-03,\n",
       "                        2.8053e-02,  2.1003e-03, -4.4497e-03, -1.1938e-02,  2.9824e-02,\n",
       "                       -8.0461e-04, -1.1597e-03, -1.1876e-02, -9.3802e-05, -6.4437e-04,\n",
       "                        3.2106e-04,  2.6143e-02, -2.4277e-04,  1.9561e-02, -1.0371e-03,\n",
       "                        2.3061e-04, -3.1302e-02, -9.7022e-04,  4.0634e-03, -4.3912e-03,\n",
       "                       -6.2894e-04,  1.5402e-03,  1.1289e-03, -2.7580e-04, -1.7148e-04,\n",
       "                        3.5765e-03,  5.8693e-03,  4.7069e-04,  1.1406e-02,  3.4716e-03,\n",
       "                        5.8435e-02, -8.8157e-04, -3.1802e-03, -1.8092e-03,  2.2880e-04,\n",
       "                        2.8980e-03,  4.0480e-03,  1.0524e-02,  6.5099e-03, -7.1963e-03,\n",
       "                        4.1086e-05,  2.1465e-03,  1.9014e-03, -9.8459e-04, -3.5630e-03,\n",
       "                        1.3344e-02,  1.7714e-02,  6.2623e-03,  4.9981e-03,  2.0930e-03,\n",
       "                        1.7877e-03,  1.2499e-03,  1.4813e-03,  3.4752e-02, -5.0252e-04,\n",
       "                        1.8965e-03,  1.0471e-03,  9.5487e-03, -2.0551e-05,  3.0876e-03,\n",
       "                       -1.0040e-02, -8.1664e-05, -7.7295e-04,  3.6393e-03,  9.5371e-04,\n",
       "                       -1.7629e-03,  3.1572e-03,  2.5211e-03, -6.1337e-04, -1.2356e-02,\n",
       "                        7.7456e-04,  3.4103e-03,  7.5800e-03,  1.3943e-03, -3.6845e-03,\n",
       "                        1.0829e-03, -8.7325e-03, -5.0183e-03, -8.5120e-04, -1.0198e-03,\n",
       "                       -2.9146e-04, -2.2830e-03, -9.9336e-04,  3.0079e-02,  2.1161e-03,\n",
       "                        9.4389e-04, -4.1442e-03,  2.1346e-03, -1.6988e-03,  1.3108e-03,\n",
       "                        9.1806e-04, -1.8346e-03,  1.3597e-03,  1.6384e-02,  2.0631e-04,\n",
       "                        7.4537e-02,  1.3574e-03,  1.6258e-03,  2.0101e-04, -1.5347e-03,\n",
       "                       -1.8739e-03,  6.6126e-03, -9.5748e-04,  1.1003e-03, -2.2316e-03,\n",
       "                       -8.3811e-04,  4.7268e-03,  7.9921e-04,  3.9590e-03, -2.2645e-03,\n",
       "                       -4.8581e-03,  1.0567e-03, -5.5840e-04,  2.9282e-03,  4.4421e-04,\n",
       "                       -2.2296e-02,  3.4965e-03,  8.1343e-04,  3.3329e-03,  2.7691e-03,\n",
       "                        1.5424e-03, -1.8469e-04, -2.6863e-03,  3.2679e-03,  2.4961e-03,\n",
       "                        2.6210e-04,  2.8120e-04, -1.0097e-03,  1.3997e-02,  1.8241e-03,\n",
       "                       -5.7885e-04,  1.4076e-03,  3.7496e-03, -2.2212e-03,  3.4602e-02,\n",
       "                       -3.4857e-03,  2.7885e-03, -1.5414e-03,  4.0945e-03,  4.3663e-04,\n",
       "                        6.0057e-04, -7.0230e-05, -2.7923e-03,  3.4579e-03,  1.3492e-03,\n",
       "                        4.9786e-04, -9.3308e-04,  2.6117e-04,  3.0536e-02,  2.2598e-03,\n",
       "                        5.1520e-02, -9.2364e-04,  7.8852e-04,  9.6568e-03, -5.2281e-03,\n",
       "                       -1.7757e-03, -2.8848e-03, -3.9324e-04, -1.4486e-03, -1.7246e-03,\n",
       "                       -3.4826e-03,  1.2125e-03,  3.0759e-02,  2.8656e-04, -4.0478e-04,\n",
       "                       -1.6923e-03, -3.7892e-04, -2.9804e-03,  1.5814e-03, -1.4071e-03,\n",
       "                        5.0414e-03, -2.6627e-03, -2.0800e-02,  2.5075e-02, -8.7714e-05,\n",
       "                       -3.5917e-03,  8.6469e-04, -8.1068e-04,  2.4430e-03, -2.4980e-04,\n",
       "                       -4.0230e-04,  1.4817e-03,  9.3374e-04,  1.5385e-03,  2.0472e-04,\n",
       "                       -5.1961e-04,  1.9485e-03,  7.3309e-03, -2.6994e-03,  4.0861e-03,\n",
       "                       -1.3333e-03, -8.2249e-04,  1.0273e-03, -2.4234e-03,  7.5330e-03,\n",
       "                       -6.3008e-03, -9.5923e-04, -3.5472e-04,  1.6039e-03, -1.3367e-03,\n",
       "                        1.4219e-02, -6.9534e-04,  4.7039e-04, -3.5088e-03, -2.0384e-03,\n",
       "                        2.2703e-04,  1.0008e-03,  5.0272e-03,  5.7809e-05, -4.0181e-04,\n",
       "                       -2.3790e-03,  2.3565e-05,  2.5604e-03, -2.4306e-03,  3.0836e-03,\n",
       "                        3.0726e-04,  3.1698e-03,  4.0007e-04,  2.2828e-03,  3.5071e-03,\n",
       "                        7.1679e-04,  1.5961e-03,  1.2362e-04,  7.0473e-03,  3.0476e-03,\n",
       "                        5.7971e-05, -3.9200e-03,  2.4797e-03,  8.1255e-03, -9.5237e-03,\n",
       "                        1.4102e-03,  6.5109e-04,  2.7948e-02, -1.6164e-01,  1.9845e-03,\n",
       "                        5.2337e-04, -1.2887e-03,  1.9347e-03, -7.7556e-04,  1.7430e-03,\n",
       "                       -2.6881e-03,  9.0835e-04, -3.6219e-03,  5.9410e-03,  3.2407e-03,\n",
       "                        2.8636e-04,  3.6184e-03, -1.2351e-02, -2.1342e-03,  3.8090e-03,\n",
       "                        4.1270e-03,  5.0829e-04,  6.7380e-02,  2.4359e-03,  2.0183e-04,\n",
       "                       -6.0530e-03, -1.0455e-02,  4.4401e-02,  5.6647e-05,  9.5071e-03,\n",
       "                        3.2854e-03, -6.0199e-03,  1.1378e-04,  2.0462e-03, -1.3683e-03,\n",
       "                       -2.1786e-03, -5.3099e-04, -1.2206e-03, -8.4531e-04,  1.4434e-01,\n",
       "                       -1.1487e-03,  3.8956e-03,  2.0973e-03,  4.8169e-03, -1.0719e-02,\n",
       "                        9.0946e-03,  7.1706e-04, -1.0307e-03,  8.1138e-03, -1.3114e-04,\n",
       "                        9.7292e-04, -5.5343e-04, -3.4328e-04, -3.7190e-02,  1.3216e-03,\n",
       "                       -8.8783e-05,  3.2431e-03, -1.7392e-01,  3.7699e-02, -5.0871e-05,\n",
       "                        3.0238e-03, -1.7306e-03,  9.7120e-05, -1.0702e-03, -1.7469e-03,\n",
       "                       -2.9956e-03, -1.3008e-03,  1.2030e-02,  5.2027e-03, -8.9121e-03,\n",
       "                       -2.5233e-04, -4.1496e-04,  3.0534e-03,  1.8612e-04,  7.4988e-04,\n",
       "                        5.8899e-04, -4.2124e-04]], device='cuda:0')),\n",
       "             ('heads.0.0.bias', tensor([0.5472], device='cuda:0')),\n",
       "             ('heads.1.0.weight',\n",
       "              tensor([[ 5.4734e-03, -9.3363e-03, -3.3032e-04, -5.9804e-03,  6.3414e-03,\n",
       "                        4.9055e-04, -9.8282e-04, -5.3706e-03,  3.7074e-03, -4.6678e-03,\n",
       "                        1.6228e-03, -3.1145e-03,  2.0978e-04, -3.6752e-02,  7.1435e-03,\n",
       "                        1.2720e-03,  5.7411e-03, -3.1562e-03, -6.1791e-03,  5.8877e-02,\n",
       "                       -7.7442e-03, -3.9719e-04,  2.3133e-03, -2.4909e-03, -3.1852e-03,\n",
       "                       -7.5341e-03,  4.8308e-03,  4.8175e-03, -8.3294e-04,  2.0404e-03,\n",
       "                        5.0972e-06, -1.0473e-02, -4.4031e-03, -1.3150e-03,  6.5764e-04,\n",
       "                        8.9867e-03,  2.3794e-03,  5.8336e-03,  7.8011e-03,  4.2178e-03,\n",
       "                       -8.7630e-04,  2.1020e-02, -1.0849e-04,  3.4450e-03,  1.4484e-03,\n",
       "                       -6.1701e-03,  8.3219e-03, -1.6820e-04,  1.9897e-03,  8.7364e-03,\n",
       "                        4.3888e-03, -1.5651e-02,  1.7379e-03,  2.7287e-03,  2.9780e-03,\n",
       "                       -1.1704e-02,  1.1099e-03, -3.8772e-03,  1.1082e-03,  9.6345e-03,\n",
       "                       -3.0710e-03,  5.2075e-03,  3.2765e-04,  9.1923e-02, -3.4940e-03,\n",
       "                        3.3159e-03, -1.0019e-02,  5.8567e-03, -4.3105e-03,  4.4913e-03,\n",
       "                        2.1113e-03, -1.1737e-03, -2.2366e-03, -4.7339e-04,  5.8711e-04,\n",
       "                       -2.4089e-03, -1.3181e-03,  3.3637e-03, -5.3895e-03,  1.7742e-02,\n",
       "                       -5.7220e-03, -3.3098e-03,  1.3541e-03, -5.5430e-04,  1.3430e-03,\n",
       "                       -3.8527e-03,  2.3659e-03, -4.3226e-03,  7.3489e-03,  4.5080e-03,\n",
       "                        9.6525e-04,  4.8074e-03, -2.5880e-03, -3.7310e-03, -1.4804e-03,\n",
       "                        2.5082e-02, -3.5099e-03, -4.1393e-03, -3.1457e-03, -4.3977e-03,\n",
       "                       -3.4363e-04, -1.1729e-03, -2.2434e-03, -1.4845e-04,  7.5996e-03,\n",
       "                        2.5569e-02,  3.8886e-03, -7.7074e-03,  2.5102e-03,  4.8309e-03,\n",
       "                       -3.1143e-02,  8.7473e-05,  4.7483e-03,  4.1349e-04,  6.4239e-04,\n",
       "                       -1.5714e-02, -4.4056e-03, -1.4445e-03, -7.3198e-03,  1.0618e-02,\n",
       "                       -5.7321e-03,  1.6449e-02,  5.4768e-03, -7.8479e-03, -7.5751e-03,\n",
       "                        5.7496e-03, -4.7134e-03,  8.5675e-04,  2.1858e-03,  3.5048e-02,\n",
       "                       -3.2055e-03,  1.7663e-03,  8.0007e-03,  6.8320e-03, -5.5818e-04,\n",
       "                        8.5184e-03,  4.9221e-04,  9.3250e-03, -1.6740e-04,  1.3002e-03,\n",
       "                       -3.6445e-03,  4.9230e-03,  9.6642e-04,  1.5150e-02, -1.9746e-03,\n",
       "                       -2.8386e-03,  9.5019e-03, -5.6698e-03, -3.6944e-03, -3.9041e-03,\n",
       "                       -2.1647e-03, -2.0486e-03, -7.0602e-03, -2.7608e-03,  3.1992e-03,\n",
       "                        4.2380e-04, -5.0281e-03, -5.8372e-03,  4.7475e-05, -7.2784e-04,\n",
       "                       -1.5217e-04, -1.2657e-02,  8.9682e-04, -8.9220e-04,  3.5348e-03,\n",
       "                       -1.0906e-03, -9.6502e-03,  3.1994e-01,  8.6621e-04, -1.9426e-02,\n",
       "                        1.3692e-03,  1.8906e-04,  4.1413e-03, -9.4918e-04, -6.9779e-03,\n",
       "                       -7.7763e-03, -3.8303e-03,  1.3333e-03,  4.5103e-03,  7.6622e-04,\n",
       "                        1.0753e-03, -1.3609e-04,  1.6436e-05, -4.1745e-03, -5.2892e-04,\n",
       "                       -2.0115e-03,  2.3614e-03,  3.4757e-03,  1.5344e-03, -1.3136e-02,\n",
       "                        2.8410e-04,  4.5596e-02, -7.1397e-04,  1.0818e-03,  1.4530e-04,\n",
       "                       -3.1394e-03,  3.0775e-03,  2.6836e-03, -4.7069e-03, -1.3349e-03,\n",
       "                        6.1604e-04, -1.2175e-03, -3.5557e-03,  4.9255e-03, -3.2377e-03,\n",
       "                        1.3241e-03,  1.7612e-03, -1.8295e-03,  5.3628e-03,  3.2344e-03,\n",
       "                        4.2855e-03, -3.4811e-03,  7.1190e-03,  6.7322e-03, -2.9186e-03,\n",
       "                       -2.4440e-04,  2.0950e-03, -7.7136e-03, -1.1783e-04,  3.8522e-03,\n",
       "                        1.6776e-03,  1.5523e-02, -4.6025e-03,  9.5684e-05, -2.8156e-03,\n",
       "                       -3.6002e-03, -1.2798e-03, -5.3659e-03, -6.0733e-04,  3.8031e-03,\n",
       "                       -3.5743e-03, -1.7029e-05, -5.7263e-03,  2.9253e-02, -2.1236e-05,\n",
       "                        2.6278e-04, -1.0358e-02, -4.1446e-03, -6.2532e-03,  2.5418e-03,\n",
       "                        4.2022e-03,  3.2833e-03, -9.2851e-03, -7.3034e-03,  3.5016e-03,\n",
       "                        1.6914e-04,  2.6701e-04,  1.0751e-03, -3.3440e-04,  3.4722e-04,\n",
       "                        1.8174e-03,  6.7523e-03,  1.7572e-03,  8.1006e-02,  5.8340e-04,\n",
       "                        1.8868e-02, -2.5007e-03,  5.5708e-03, -4.3018e-02,  1.5214e-03,\n",
       "                        1.8668e-03, -1.0109e-03,  2.9848e-03, -2.7263e-04, -5.1338e-03,\n",
       "                        1.0677e-03, -4.3159e-03,  1.1709e-03,  8.5583e-03,  3.9138e-03,\n",
       "                        6.8543e-03,  1.7253e-03,  3.6318e-03, -3.1050e-03,  9.6531e-03,\n",
       "                       -1.1748e-03, -2.9694e-03,  5.0340e-02,  2.3117e-03, -3.2260e-03,\n",
       "                        2.6711e-03,  4.9573e-02, -7.1135e-03,  3.2095e-03,  2.4863e-03,\n",
       "                        2.7651e-02,  4.7900e-03,  3.4513e-03,  9.0547e-02, -5.6585e-03,\n",
       "                        1.0046e-04, -4.6217e-03, -1.8714e-03,  4.1804e-04,  5.7243e-03,\n",
       "                        5.2957e-02, -2.6843e-03,  1.0955e-02, -1.9391e-02, -3.3249e-03,\n",
       "                        5.0149e-03,  5.2692e-03,  1.0437e-03, -2.7185e-03, -3.5065e-04,\n",
       "                       -9.9834e-05, -2.3552e-02, -4.0013e-03,  6.1410e-03, -1.0515e-03,\n",
       "                       -1.7897e-03, -3.4055e-03,  2.6318e-03,  3.9289e-03, -4.2169e-04,\n",
       "                        5.4764e-03, -6.8881e-04,  3.7189e-03,  8.4811e-05, -2.0193e-03,\n",
       "                        1.5724e-02, -1.6120e-03,  2.2108e-03,  2.7018e-03,  1.8841e-03,\n",
       "                       -4.0455e-03,  9.6304e-05,  2.8668e-03,  2.7143e-04,  1.9542e-03,\n",
       "                        8.4575e-03, -8.0943e-04,  3.5312e-03, -2.4851e-02,  4.0654e-04,\n",
       "                       -7.2434e-04,  3.2859e-03,  1.7935e-02,  4.2792e-03,  8.3789e-05,\n",
       "                        7.8617e-04,  9.2799e-03,  4.0055e-03, -1.3244e-01,  4.5989e-03,\n",
       "                       -1.1338e-02, -9.8843e-04,  7.2745e-04,  2.7950e-04,  2.3573e-03,\n",
       "                        5.6301e-03,  1.7838e-03,  1.4871e-02, -3.4271e-04, -4.3168e-03,\n",
       "                       -8.4176e-04,  6.1067e-04,  1.5094e-03, -8.2248e-03,  4.1057e-03,\n",
       "                        5.6252e-03,  2.0500e-03, -1.1776e-03, -3.9875e-03, -2.8888e-04,\n",
       "                       -1.4746e-03, -2.2856e-04,  3.3873e-03, -3.0929e-03, -7.8889e-03,\n",
       "                       -3.1419e-03,  6.4855e-04,  1.9057e-04,  3.9465e-02, -5.6903e-03,\n",
       "                       -3.1495e-03,  3.4402e-03,  4.6513e-03,  3.6202e-03, -3.8645e-03,\n",
       "                       -7.5527e-03, -2.3012e-04, -1.2429e-03,  1.6071e-03,  1.1835e-02,\n",
       "                       -6.2364e-04,  1.1609e-03,  4.1123e-04, -4.8114e-03, -3.8702e-03,\n",
       "                        6.9583e-03, -4.6559e-04, -3.6274e-03, -5.5314e-03, -1.4680e-03,\n",
       "                       -4.1854e-03,  3.5052e-03, -8.1718e-03, -3.5183e-03, -3.3670e-03,\n",
       "                        3.9639e-02,  9.1792e-04,  1.0954e-02, -4.0456e-03, -1.8536e-03,\n",
       "                        2.4269e-02,  5.1975e-04, -2.8094e-03,  4.3528e-03, -2.1322e-04,\n",
       "                       -4.3415e-04,  1.6132e-04,  8.5633e-04,  1.4711e-03,  1.0956e-04,\n",
       "                       -1.4263e-03,  1.7609e-03,  1.6480e-03,  3.9593e-03,  3.5774e-03,\n",
       "                       -3.4149e-03,  1.7013e-03, -4.0092e-04, -1.8680e-03,  4.0513e-04,\n",
       "                        2.2356e-03,  3.5909e-03, -3.8224e-03,  3.1519e-02,  4.2250e-03,\n",
       "                       -2.4467e-04, -1.8792e-03, -3.5756e-03, -2.1815e-02,  5.7043e-03,\n",
       "                        1.8917e-03,  9.0205e-07,  7.0500e-02, -4.4015e-03,  8.8412e-03,\n",
       "                       -3.8209e-03,  1.0736e-02, -8.5313e-03, -3.9051e-03, -3.8951e-04,\n",
       "                        9.5291e-03, -3.4916e-04,  5.0727e-04,  5.9188e-03, -5.3262e-03,\n",
       "                        2.9245e-03, -1.1824e-03,  2.0208e-02, -5.7820e-03, -1.2710e-02,\n",
       "                        1.7313e-02,  3.5761e-03, -2.1865e-03,  5.7863e-03,  2.2267e-04,\n",
       "                        1.4860e-03,  1.1830e-02,  1.1584e-03,  2.5834e-03,  5.7860e-03,\n",
       "                       -5.5461e-03,  3.6593e-03, -5.0549e-04,  7.4942e-03,  2.9817e-03,\n",
       "                        5.6760e-03,  5.1807e-03, -1.5064e-04,  3.6536e-04,  2.8459e-02,\n",
       "                        2.7831e-03, -1.0138e-02,  6.9045e-03,  4.8284e-03,  7.3058e-03,\n",
       "                        2.5989e-02,  4.4848e-03,  1.7399e-03, -1.1108e-02,  1.6323e-03,\n",
       "                        4.6497e-04,  5.1639e-03, -6.2386e-03, -5.6300e-02, -2.9248e-03,\n",
       "                       -2.9854e-04, -2.5774e-03, -6.8058e-03, -8.9131e-03, -4.3704e-04,\n",
       "                        5.4537e-03, -3.7607e-03,  1.4837e-03,  2.9773e-03, -9.9079e-03,\n",
       "                        3.7515e-03, -7.2576e-03,  9.2287e-02, -5.5309e-03,  1.9429e-03,\n",
       "                       -9.5013e-03,  4.0943e-03,  1.5115e-03,  1.0747e-03, -4.2227e-03,\n",
       "                        4.4737e-03,  8.9976e-05]], device='cuda:0')),\n",
       "             ('heads.1.0.bias', tensor([0.3721], device='cuda:0'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixDataset(Dataset):\n",
    "    def __init__(self, json_file, class_mapping, max_seq_len, bins_file, label_str, transform=None):\n",
    "        \"\"\"\n",
    "        :param json_file: 存储数据的JSON文件路径\n",
    "        :param class_mapping: 类别名到整数标签的映射\n",
    "        :param nprint_width: nprint的固定宽度\n",
    "        :param transform: 图像预处理转换\n",
    "        \"\"\"\n",
    "        self.json_file = json_file\n",
    "        self.class_mapping = class_mapping  # 类别映射\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.label_str = label_str\n",
    "        label_int = self.class_mapping[label_str]\n",
    "        self.label_one_hot = F.one_hot(torch.tensor(label_int), num_classes=len(self.class_mapping)).float()\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 读取JSON文件\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)['data']\n",
    "        \n",
    "        with open(bins_file, 'r') as f_bin:\n",
    "            self.bins_data = json.load(f_bin)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        def find_interval(value, intervals):\n",
    "            for idx, [start, end] in enumerate(intervals):\n",
    "                if start <= value <= end:\n",
    "                    return idx  # 返回所在区间的下标\n",
    "            return None\n",
    "        # 解析每一条数据\n",
    "        item = self.data[idx]\n",
    "\n",
    "        metadata = np.array(list(item['meta'].values()), dtype=np.float32)\n",
    "        length = min(metadata[1],self.max_seq_len)\n",
    "\n",
    "        labelstr = item['labels'][0]  # 假设 labels 是字符串类型\n",
    "        \n",
    "        is_real = np.zeros(2)\n",
    "        \n",
    "        if labelstr == self.label_str:\n",
    "            is_real[0] = 1\n",
    "        else:\n",
    "            is_real[1] = 1\n",
    "        #     print(labelstr,self.label_str)\n",
    "        # print(is_real)\n",
    "        \n",
    "\n",
    "        port_intervals = self.bins_data['port']['intervals']\n",
    "        pkt_len_intervals = []\n",
    "        for bins in self.bins_data['packet_len']:\n",
    "            pkt_len_intervals.append(bins['intervals'])\n",
    "        time_intervals = []\n",
    "        for bins in self.bins_data['time']:\n",
    "            time_intervals.append(bins['intervals'])\n",
    "\n",
    "        seq = []\n",
    "\n",
    "        im = bytes.fromhex(item['nprint'])\n",
    "        # def split_bytes_by_length(data, chunk_size):\n",
    "        #     return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "        # lines = split_bytes_by_length(im, NPRINT_LINE_LEN)\n",
    "\n",
    "        line = im[0:TOTAL_LEN]\n",
    "        tcp_dport = line[32:34]\n",
    "        udp_dport = line[92:94]\n",
    "        dport = bytearray(a | b for a, b in zip(tcp_dport, udp_dport))\n",
    "        dport = int.from_bytes(dport, 'big')\n",
    "        \n",
    "        \n",
    "        dport_id = find_interval(dport,port_intervals)\n",
    "        dport = dport_id/len(port_intervals)\n",
    "        # dport /= MAX_PORT\n",
    "        dport = dport * 2 - 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        for i in range(0, len(im), TOTAL_LEN):\n",
    "            # new_line = line[:22]+line[34:46]+line[98:]\n",
    "            # line = bytes(line) \n",
    "            line = im[i:i+TOTAL_LEN]\n",
    "            # print(line[0:8])\n",
    "            time_h,time_l, pkt_len = struct.unpack(\"IIh\", line[:10])\n",
    "            time_l //= 1e4\n",
    "            time = time_h + time_l/100\n",
    "            \n",
    "            time_id = find_interval(time,time_intervals[count])\n",
    "            pkt_len_id = find_interval(pkt_len,pkt_len_intervals[count])\n",
    "            # sign = -1\n",
    "            \n",
    "            # if pkt_len < 0:\n",
    "            #     sign = 1\n",
    "            #     pkt_len = -pkt_len\n",
    "            \n",
    "            time = time_id/len(time_intervals[count])\n",
    "            pkt_len = pkt_len_id/len(pkt_len_intervals[count])\n",
    "            \n",
    "            time = time * 2 - 1\n",
    "            pkt_len = pkt_len * 2 - 1\n",
    "            \n",
    "            seq.append([time,pkt_len,dport])\n",
    "            count += 1\n",
    "            if count >= self.max_seq_len:\n",
    "                break\n",
    "        \n",
    "        # 填充 nprint，使其宽度固定\n",
    "        if len(seq) < self.max_seq_len:\n",
    "            seq = np.pad(seq, ((0, self.max_seq_len - len(seq)), (0, 0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        \n",
    "        # 转换为PyTorch的Tensor\n",
    "        labels_one_hot = torch.tensor(self.label_one_hot, dtype=torch.float32)  # 转换为Tensor\n",
    "        seq = torch.tensor(seq, dtype=torch.float32)\n",
    "        length = torch.tensor(length, dtype=torch.float32)\n",
    "        is_real = torch.tensor(is_real, dtype=torch.float32)\n",
    "        \n",
    "        return seq, labels_one_hot, length, is_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(label_str):\n",
    "    dataset = MixDataset(source_name,LABEL_DICT,MAX_SEQ_LEN,bins_name,label_str)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "    real_score = 0\n",
    "    fake_score = 0\n",
    "    total_score = 0\n",
    "    \n",
    "    valid_list = []\n",
    "    real_list = []\n",
    "    with torch.no_grad():\n",
    "        for seqs, labels, lengths, is_reals in dataloader:\n",
    "            seqs = seqs.to(torch.device(\"cpu\"))\n",
    "            lengths = lengths.to(torch.device(\"cpu\"))  # 确保在同一个设备上\n",
    "            labels = labels.to(torch.device(\"cpu\"))\n",
    "            is_reals = is_reals.to(torch.device(\"cpu\"))\n",
    "            # 生成随机噪声向量\n",
    "            # noise = torch.randn(len(lengths), noise_dim)\n",
    "            # 输入生成器生成数据\n",
    "            # fake_data = generator(labels, noise, lengths)\n",
    "            validity = discriminator(labels, seqs, lengths)\n",
    "            \n",
    "            validity = torch.sigmoid(validity).squeeze(1)\n",
    "            \n",
    "            real_score += torch.sum(validity * (is_reals[:,0]))\n",
    "            fake_score += torch.sum(validity * (is_reals[:,1]))\n",
    "            total_score += torch.sum(validity)\n",
    "            \n",
    "            valid_list += validity.tolist()\n",
    "            real_list += is_reals[:,0].tolist()\n",
    "    \n",
    "    auc = roc_auc_score(real_list, valid_list)\n",
    "            \n",
    "    return real_score,fake_score, total_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292256/2675908802.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_one_hot = torch.tensor(self.label_one_hot, dtype=torch.float32)  # 转换为Tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:1991.2022705078125 Fake:1528.9072265625 Total:3520.10986328125 AUC:0.303364259508925\n",
      "skype :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292256/2675908802.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_one_hot = torch.tensor(self.label_one_hot, dtype=torch.float32)  # 转换为Tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:1624.4080810546875 Fake:2048.10791015625 Total:3672.51611328125 AUC:0.7923031624673058\n"
     ]
    }
   ],
   "source": [
    "for label in LABEL_DICT.keys():\n",
    "    print(label,\":\")\n",
    "    real_score,fake_score,total_score,auc = test_data(label)\n",
    "    print(f\"Real:{real_score}\",f\"Fake:{fake_score}\",f\"Total:{total_score}\",f\"AUC:{auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
