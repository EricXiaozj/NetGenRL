{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "from seqCGAN.generator import Generator  # 假设你有一个定义好的 Discriminator 类\n",
    "from seqCGAN.util import *\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import struct\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from dtaidistance import dtw\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL_LEN = 114\n",
    "# # NPRINT_REAL_WIDTH = 50*8\n",
    "# NPRINT_REAL_WIDTH = 22*8\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'aim': 2, 'email': 3, 'voipbuster': 4, 'hangouts': 5, 'youtube': 6, 'sftp': 7, 'icq': 8,  'ftps': 9, 'vimeo': 10, 'spotify': 11, 'netflix': 12, 'bittorrent': 13}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'hangouts': 4, 'youtube': 5, 'ftps': 6, 'vimeo': 7, 'spotify': 8, 'netflix': 9, 'bittorrent': 10}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'youtube': 4, 'ftps': 5, 'vimeo': 6, 'spotify': 7, 'netflix': 8, 'bittorrent': 9}\n",
    "# LABEL_DICT = {'email': 0, 'youtube': 1, 'ftps': 2, 'vimeo': 3, 'spotify': 4, 'netflix': 5, 'bittorrent': 6}\n",
    "\n",
    "# SEQ_DIM = 2\n",
    "# MAX_PKT_LEN = 3001\n",
    "# MAX_TIME = 10000\n",
    "# MAX_PORT = 65535\n",
    "# MAX_SEQ_LEN = 16\n",
    "# WORD_VEC_SIZE = 8\n",
    "# SEQ_DIM = WORD_VEC_SIZE * 2 + 1\n",
    "\n",
    "label_dim = len(LABEL_DICT) \n",
    "# image_dim = (1, NPRINT_REAL_WIDTH, NPRINT_REAL_WIDTH)  # 生成单通道图像\n",
    "noise_dim = 100  # 噪声维度\n",
    "batch_size = 128\n",
    "# source_name = './data/vpn_data_small.json'\n",
    "# bins_name = './bins/bins_small_new.json'\n",
    "data_folder = './data/' + DATASET + '/'\n",
    "bins_file_name = './bins/bins_' + DATASET + '.json'\n",
    "wordvec_file_name = './wordvec/word_vec_' + DATASET + '.json'\n",
    "model_name = './save_seq/generator.pth'\n",
    "\n",
    "with open(wordvec_file_name, 'r') as f:\n",
    "    wv_dict = json.load(f)\n",
    "    \n",
    "wv = {}\n",
    "for key, metrics in wv_dict.items():\n",
    "    wv[key] = torch.tensor(metrics, dtype=torch.float32)\n",
    "\n",
    "x_list = [wv_tensor.size(0) for wv_tensor in wv.values()]\n",
    "# x_list = [MAX_TIME, MAX_PKT_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_data = {}\n",
    "with open(bins_file_name, 'r') as f_bin:\n",
    "    bins_data = json.load(f_bin)\n",
    "    \n",
    "# port_intervals = bins_data['port']['intervals']\n",
    "# pkt_len_intervals = bins_data['packet_len']['intervals']\n",
    "# time_intervals = bins_data['time']['intervals']\n",
    "# for bins in bins_data['packet_len']:\n",
    "#     pkt_len_intervals.append(bins['intervals'])\n",
    "# time_intervals = []\n",
    "# for bins in bins_data['time']:\n",
    "#     time_intervals.append(bins['intervals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = Generator(label_dim, noise_dim, SEQ_DIM, MAX_SEQ_LEN, 'cpu')\n",
    "generator = Generator(label_dim,SEQ_DIM,MAX_SEQ_LEN,x_list,'cpu')\n",
    "\n",
    "# 加载模型权重\n",
    "checkpoint = torch.load(model_name)  # 加载保存的权重字典\n",
    "generator.load_state_dict(checkpoint)  # 将权重字典加载到模型中\n",
    "\n",
    "# 切换到评估模式\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in generator.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_data(file_name):\n",
    "    \n",
    "    data_dic = {}\n",
    "    for filename in os.listdir(data_folder):\n",
    "        with open(data_folder + filename, 'r') as f:\n",
    "            if filename.split('.')[0] not in LABEL_DICT.keys():\n",
    "                continue\n",
    "            json_data = json.load(f)\n",
    "            data_dic[filename.split('.')[0]] = []\n",
    "            for item in json_data:\n",
    "                meta_list = []\n",
    "                for meta_attr in META_LIST:\n",
    "                    meta_list.append(item[meta_attr])\n",
    "                count = 0\n",
    "                seq = []\n",
    "                for pkt in item['series']:\n",
    "                    attr_list = []\n",
    "                    for sery_attr in SERY_LIST:\n",
    "                        attr_list.append(pkt[sery_attr])\n",
    "                    seq.append(attr_list + meta_list)\n",
    "                    count += 1\n",
    "                    if count >= MAX_SEQ_LEN:\n",
    "                        break\n",
    "                data_dic[filename.split('.')[0]].append(seq)\n",
    "            # data_dic[filename.split('.')[0]] = json_data['data']\n",
    "            # data += json_data\n",
    "            # labels += [filename.split('.')[0]] * len(json_data)\n",
    "    # with open(file_name,'r') as f:\n",
    "    #     json_data = json.load(f)['data']\n",
    "    # data_dic = {}\n",
    "    \n",
    "    # for item in json_data:\n",
    "    #     label_str = item['labels'][0]\n",
    "    #     if label_str not in data_dic:\n",
    "    #         data_dic[label_str] = []\n",
    "\n",
    "    #     packets_len = item['meta']['packets']\n",
    "    #     im = bytes.fromhex(item['nprint'])\n",
    "\n",
    "    #     line = im[0:TOTAL_LEN]\n",
    "    #     tcp_dport = line[32:34]\n",
    "    #     udp_dport = line[92:94]\n",
    "\n",
    "    #     dport = bytearray(a | b for a, b in zip(tcp_dport, udp_dport))\n",
    "    #     dport = int.from_bytes(dport, 'big')\n",
    "    \n",
    "    #     data_item = []\n",
    "    #     for i in range(min(16,packets_len)):\n",
    "    #         line = im[i*TOTAL_LEN:i*TOTAL_LEN+TOTAL_LEN]\n",
    "    #         # time_h, time_l, pl = struct.unpack(\"IIh\", line[:10])\n",
    "    #         # time_l //= 1e4\n",
    "    #         # time = time_h + time_l/100\n",
    "    #         time_h,time_l, pkt_len = struct.unpack(\"IIh\", line[:10])\n",
    "    #         time_l //= 1e4\n",
    "    #         time = time_h + time_l/100\n",
    "    #         time = time_h % 1000 + time_l / 10\n",
    "            \n",
    "    #         data_item.append([time, pkt_len])\n",
    "            \n",
    "    #     data_dic[label_str].append(data_item)\n",
    "        \n",
    "    return data_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_datas = get_real_data(data_folder)\n",
    "print(real_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datas, label_str):\n",
    "        \"\"\"\n",
    "        :param sequences: 一个包含真实序列的列表，每个序列是一个 ndarray 或 list\n",
    "        \"\"\"\n",
    "        self.datas = datas\n",
    "        self.lengths = [len(seq) for seq in datas]  # 提取序列长度\n",
    "        label_id = LABEL_DICT[label_str]\n",
    "        self.label_one_hot = torch.zeros(label_dim).to('cpu') \n",
    "        self.label_one_hot[label_id] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lengths[idx],self.label_one_hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_data(real_data, label_str):\n",
    "    dataset = SequenceDataset(real_data,label_str)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "    generated_sequences = []\n",
    "    with torch.no_grad():\n",
    "        for lengths, labels in dataloader:\n",
    "            lengths = lengths.to(torch.device(\"cpu\"))  # 确保在同一个设备上\n",
    "            labels = labels.to(torch.device(\"cpu\"))\n",
    "            # print(lengths.shape)\n",
    "            # print(labels.shape)\n",
    "            batch_size = lengths.size(0)\n",
    "            # 生成随机噪声向量\n",
    "            # noise = torch.randn(len(lengths), noise_dim)\n",
    "            # 输入生成器生成数据\n",
    "            fake_data = generator.sample(batch_size,labels,lengths)\n",
    "            # 将生成结果按序列长度截断\n",
    "            for i, length in enumerate(lengths):\n",
    "                generated_sequences.append(fake_data[i, :length].cpu().tolist())\n",
    "       \n",
    "    final_seqs = []         \n",
    "    for seq in generated_sequences:\n",
    "        f_seq = []\n",
    "        for i in range(len(seq)):\n",
    "            pkt = []\n",
    "            for j,attr_id in enumerate(seq[i]): \n",
    "                if j == 0:\n",
    "                    attr = round(random.uniform(bins_data[SEQ_LIST[j]]['intervals'][attr_id][0], bins_data[SEQ_LIST[j]]['intervals'][attr_id][1]),2)\n",
    "                else:\n",
    "                    attr = round(random.uniform(bins_data[SEQ_LIST[j]]['intervals'][attr_id][0], bins_data[SEQ_LIST[j]]['intervals'][attr_id][1]))\n",
    "                pkt.append(attr)\n",
    "            # [time,pkt_len] = seq[i]       \n",
    "            # # time_id = math.floor((time + 1)/2*len(time_intervals[i]))\n",
    "            # # pkt_len_id = math.floor((pkt_len + 1)/2*len(pkt_len_intervals[i]))\n",
    "            # # dport_id = math.floor((dport + 1)/2*len(port_intervals))\n",
    "        \n",
    "            # time = round(random.uniform(time_intervals[time][0], time_intervals[time][1]),2)\n",
    "            # pkt_len = round(random.uniform(pkt_len_intervals[pkt_len][0], pkt_len_intervals[pkt_len][1]))\n",
    "            # dport = round(random.uniform(port_intervals[dport_id][0],port_intervals[dport_id][1]))\n",
    "            # time = time\n",
    "            # pkt_len = pkt_len\n",
    "            # f_seq.append([time,pkt_len])\n",
    "            f_seq.append(pkt)\n",
    "        final_seqs.append(f_seq)\n",
    "    return final_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sequence, target_length, pad_value=np.nan):\n",
    "    seq_len = len(sequence)\n",
    "    if seq_len < target_length:\n",
    "        padding = [[pad_value] * len(sequence[0])] * (target_length - seq_len)\n",
    "        return sequence + padding  # 填充\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas = {}\n",
    "for label, data in real_datas.items():\n",
    "    # fake_data = get_fake_data_wv(data,label,wv)\n",
    "    fake_data = get_fake_data(data,label)\n",
    "    fake_datas[label] = fake_data\n",
    "    # for i in range(1):\n",
    "    #     fake_data = get_fake_data(data,label)\n",
    "    #     fake_datas[label] += fake_data\n",
    "    # print(label)\n",
    "    # print(data)\n",
    "    # print(fake_data)\n",
    "    # real_grouped = group_by_length(data)\n",
    "    # fake_grouped = group_by_length(fake_data)\n",
    "    # overall_js_divergence = calculate_divergence(real_grouped, fake_grouped)\n",
    "    # print(f\"Overall JS Divergence {label}: {overall_js_divergence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in LABEL_DICT.keys():\n",
    "    real_data = real_datas[label]\n",
    "    fake_data = fake_datas[label]\n",
    "    real_sequences = np.array([pad(seq, MAX_SEQ_LEN) for seq in real_data])         # Shape: (num_samples, seq_len, num_dims)\n",
    "    generated_sequences = np.array([pad(seq, MAX_SEQ_LEN) for seq in fake_data]) \n",
    "\n",
    "    num_samples, seq_len, num_dims = real_sequences.shape\n",
    "    # print(real_sequences.shape)\n",
    "\n",
    "    for dim in range(num_dims):\n",
    "        \n",
    "        percentiles = [10, 90]\n",
    "        quantiles = np.nanpercentile(real_sequences[:,:,dim], percentiles, axis=0)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if dim == 0:\n",
    "            plt.axis([0, 16, 0, 3000])\n",
    "        elif dim == 1:\n",
    "            plt.axis([0, 16, -1500, 1500])\n",
    "        elif dim == 2 or dim == 3:\n",
    "            plt.axis([0, 16, 0, 255])\n",
    "        elif dim == 4 or dim == 5:\n",
    "            plt.axis([0, 16, 0, 65535])\n",
    "        else:\n",
    "            plt.axis([0, 16, 0, 4294967295])\n",
    "\n",
    "        # 绘制真实数据序列（统一颜色）\n",
    "        for i in range(len(real_sequences)):\n",
    "            plt.plot(real_sequences[i, :, dim], color='blue', alpha=1.0, label=\"Real\" if i == 0 else \"\")  # 使用蓝色，透明度为 0.6\n",
    "    \n",
    "        # 绘制生成数据序列（统一颜色）\n",
    "        for i in range(len(generated_sequences)):\n",
    "            plt.plot(generated_sequences[i, :, dim], color='red', alpha=0.5, linestyle='--', label=\"Generated\" if i == 0 else \"\")  # 使用红色，透明度为 0.6\n",
    "            \n",
    "        # for q in quantiles:\n",
    "        #     plt.plot(q, color='green', linestyle='--', linewidth=2.5, label='Real Data Quantiles' if 'Real Data Quantiles' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "    \n",
    "        plt.title(f\"Dimension {dim + 1} Comparison of {label}\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot  # optimal transport library\n",
    "\n",
    "def hamming_distance_matrix(X, Y):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    batch_size = len(X)\n",
    "    cost_matrix = np.zeros((batch_size, batch_size))\n",
    "    for i in range(batch_size):\n",
    "        for j in range(batch_size):\n",
    "            cost_matrix[i, j] = np.sum(X[i] != Y[j])\n",
    "    return cost_matrix\n",
    "\n",
    "\n",
    "X = np.array([\n",
    "    [[0, 1], [1, 0]],  # 样本 1\n",
    "    [[2, 3], [3, 2]]   # 样本 2\n",
    "])\n",
    "Y1 = np.array([\n",
    "    [[2, 3], [3, 2]],  # 样本 1\n",
    "    [[0, 1], [1, 0]]   # 样本 2\n",
    "])\n",
    "Y2 = np.array([\n",
    "    [[100, 101], [101, 100]],  # 样本 1\n",
    "    [[-98, -97], [-97, -98]]   # 样本 2\n",
    "])\n",
    "\n",
    "# 计算 X 和 Y 之间所有样本对的距离矩阵\n",
    "# cost_matrix_Y1 = np.linalg.norm(X[:, None, :, :] - Y1[None, :, :, :], axis=(-2, -1))\n",
    "# cost_matrix_Y2 = np.linalg.norm(X[:, None, :, :] - Y2[None, :, :, :], axis=(-2, -1))\n",
    "cost_matrix_Y1 = hamming_distance_matrix(X, Y1)\n",
    "cost_matrix_Y2 = hamming_distance_matrix(X, Y2)\n",
    "\n",
    "# 计算最优传输距离\n",
    "ot_distance_Y1 = ot.emd2([], [], cost_matrix_Y1)  # Optimal Transport Distance\n",
    "ot_distance_Y2 = ot.emd2([], [], cost_matrix_Y2)\n",
    "\n",
    "print(\"OT 距离 (X vs Y1, 预期相同):\", ot_distance_Y1)  # 应该接近 0\n",
    "print(\"OT 距离 (X vs Y2, 预期不同):\", ot_distance_Y2)  # 应该很大\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal transport distance\n",
    "for label in LABEL_DICT.keys():\n",
    "    real_data = real_datas[label]\n",
    "    fake_data = fake_datas[label]\n",
    "    real_sequences = np.array([pad(seq, MAX_SEQ_LEN) for seq in real_data])         # Shape: (num_samples, seq_len, num_dims)\n",
    "    generated_sequences = np.array([pad(seq, MAX_SEQ_LEN) for seq in fake_data]) \n",
    "\n",
    "    num_samples, seq_len, num_dims = real_sequences.shape\n",
    "    # print(real_sequences.shape)\n",
    "\n",
    "    # for dim in range(num_dims):\n",
    "    X = real_sequences\n",
    "    Y = generated_sequences\n",
    "        \n",
    "        # Z = np.random.rand(Y.shape[0], Y.shape[1])  # 随机生成一个矩阵\n",
    "        \n",
    "    X_filled = np.nan_to_num(X, nan=10000)  # 或者 nan=0，如果认为0不影响\n",
    "    Y_filled = np.nan_to_num(Y, nan=10000)\n",
    "    \n",
    "    # X_filled = X_filled / 1500\n",
    "    # Y_filled = Y_filled / 1500\n",
    "        \n",
    "    # cost_matrix = np.linalg.norm(np.cumsum(X, axis=0)[:, None, :] - np.cumsum(Y, axis=0)[None, :, :], axis=-1)\n",
    "    cost_matrix = np.linalg.norm(X_filled[:, None, :, :] - Y_filled[None, :, :, :], axis=(-2, -1))\n",
    "        \n",
    "    # print(cost_matrix.shape)\n",
    "    ot_distance = ot.emd2([], [], cost_matrix) \n",
    "        \n",
    "    print(f\"OT 距离 {label} {dim}:\", ot_distance)\n",
    "        \n",
    "        # percentiles = [10, 90]\n",
    "        # quantiles = np.nanpercentile(real_sequences[:,:,dim], percentiles, axis=0)\n",
    "        \n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # if dim == 0:\n",
    "        #     plt.axis([0, 16, 0, 3000])\n",
    "        # else:\n",
    "        #     plt.axis([0, 16, -1500, 1500])\n",
    "\n",
    "        # # 绘制真实数据序列（统一颜色）\n",
    "        # for i in range(len(real_sequences)):\n",
    "        #     plt.plot(real_sequences[i, :, dim], color='blue', alpha=1.0, label=\"Real\" if i == 0 else \"\")  # 使用蓝色，透明度为 0.6\n",
    "    \n",
    "        # # 绘制生成数据序列（统一颜色）\n",
    "        # for i in range(len(generated_sequences)):\n",
    "        #     plt.plot(generated_sequences[i, :, dim], color='red', alpha=0.5, linestyle='--', label=\"Generated\" if i == 0 else \"\")  # 使用红色，透明度为 0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
