{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "from seqCGAN.generator import Generator  # 假设你有一个定义好的 Discriminator 类\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import struct\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_LEN = 114\n",
    "# NPRINT_REAL_WIDTH = 50*8\n",
    "NPRINT_REAL_WIDTH = 22*8\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'aim': 2, 'email': 3, 'voipbuster': 4, 'hangouts': 5, 'youtube': 6, 'sftp': 7, 'icq': 8,  'ftps': 9, 'vimeo': 10, 'spotify': 11, 'netflix': 12, 'bittorrent': 13}\n",
    "LABEL_DICT = {'facebook': 0, 'skype': 1}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'hangouts': 4, 'youtube': 5, 'ftps': 6, 'vimeo': 7, 'spotify': 8, 'netflix': 9, 'bittorrent': 10}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'youtube': 4, 'ftps': 5, 'vimeo': 6, 'spotify': 7, 'netflix': 8, 'bittorrent': 9}\n",
    "# LABEL_DICT = {'email': 0, 'youtube': 1, 'ftps': 2, 'vimeo': 3, 'spotify': 4, 'netflix': 5, 'bittorrent': 6}\n",
    "\n",
    "SEQ_DIM = 2\n",
    "MAX_PKT_LEN = 3001\n",
    "MAX_TIME = 10000\n",
    "MAX_PORT = 65535\n",
    "MAX_SEQ_LEN = 16\n",
    "WORD_VEC_SIZE = 8\n",
    "# SEQ_DIM = WORD_VEC_SIZE * 2 + 1\n",
    "\n",
    "label_dim = len(LABEL_DICT) \n",
    "image_dim = (1, NPRINT_REAL_WIDTH, NPRINT_REAL_WIDTH)  # 生成单通道图像\n",
    "noise_dim = 100  # 噪声维度\n",
    "batch_size = 128\n",
    "source_name = '../data/vpn_data_small.json'\n",
    "source_dir = '../data/netshare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_data(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        json_data = json.load(f)['data']\n",
    "    data_dic = {}\n",
    "    \n",
    "    for item in json_data:\n",
    "        label_str = item['labels'][0]\n",
    "        if label_str not in data_dic:\n",
    "            data_dic[label_str] = []\n",
    "\n",
    "        packets_len = item['meta']['packets']\n",
    "        im = bytes.fromhex(item['nprint'])\n",
    "\n",
    "        line = im[0:TOTAL_LEN]\n",
    "        tcp_dport = line[32:34]\n",
    "        udp_dport = line[92:94]\n",
    "\n",
    "        dport = bytearray(a | b for a, b in zip(tcp_dport, udp_dport))\n",
    "        dport = int.from_bytes(dport, 'big')\n",
    "    \n",
    "        data_item = []\n",
    "        for i in range(min(16,packets_len)):\n",
    "            line = im[i*TOTAL_LEN:i*TOTAL_LEN+TOTAL_LEN]\n",
    "            # time_h, time_l, pl = struct.unpack(\"IIh\", line[:10])\n",
    "            # time_l //= 1e4\n",
    "            # time = time_h + time_l/100\n",
    "            time_h,time_l, pkt_len = struct.unpack(\"IIh\", line[:10])\n",
    "            time_l //= 1e4\n",
    "            time = time_h + time_l/100\n",
    "            time = time_h % 1000 + time_l / 10\n",
    "            \n",
    "            data_item.append([time, pkt_len])\n",
    "            \n",
    "        data_dic[label_str].append(data_item)\n",
    "        \n",
    "    return data_dic\n",
    "\n",
    "def get_fake_data(source_dir, label_dict):\n",
    "    final_seqs = {}\n",
    "    for label_name in label_dict.keys():\n",
    "        filename = source_dir + '/' + label_name + '.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "    \n",
    "        flows = {}      \n",
    "        for index, row in df.iterrows():\n",
    "            flow_o = (row['srcip'], row['srcport'], row['dstip'], row['dstport'], row['proto'])\n",
    "        # flow_p = (row['dstip'], row['dstport'], row['srcip'], row['srcport'], row['proto'])\n",
    "        \n",
    "            time = round(row['time'])/1e6\n",
    "            pkt_len = round(row['pkt_len'])\n",
    "        \n",
    "            if flow_o not in flows:\n",
    "                flows[flow_o] = [[time,pkt_len]]\n",
    "                continue\n",
    "            if flow_o in flows:\n",
    "                flows[flow_o].append([time,pkt_len])\n",
    "                continue\n",
    "    \n",
    "        final_seqs[label_name] = list(flows.values())\n",
    "        for i in range(len(final_seqs[label_name])):\n",
    "            tmp = final_seqs[label_name][i][0][0]\n",
    "            now = 0\n",
    "            for j in range(len(final_seqs[label_name][i])):\n",
    "                now = final_seqs[label_name][i][j][0]\n",
    "                final_seqs[label_name][i][j][0] -= tmp\n",
    "                tmp = now\n",
    "    return final_seqs\n",
    "\n",
    "def pad(sequence, target_length, pad_value=np.nan):\n",
    "    seq_len = len(sequence)\n",
    "    if seq_len < target_length:\n",
    "        padding = [[pad_value] * len(sequence[0])] * (target_length - seq_len)\n",
    "        return sequence + padding  # 填充\n",
    "    return sequence\n",
    "\n",
    "def get_training_dataset(data_dic):\n",
    "    x = []\n",
    "    y = []\n",
    "    label_count = 0\n",
    "    for label, data in data_dic.items():\n",
    "        x.extend(data)  # 添加所有序列\n",
    "        y.extend([label_count] * len(data))\n",
    "        label_count += 1\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_datas = get_real_data(source_name)\n",
    "\n",
    "fake_datas = get_fake_data(source_dir, LABEL_DICT)\n",
    "\n",
    "# fake_datas = {}\n",
    "# for label, data in real_datas.items():\n",
    "#     fake_data = get_fake_data(data,label)\n",
    "#     fake_datas[label] = fake_data\n",
    "    \n",
    "x_real, y_real = get_training_dataset(real_datas)\n",
    "x_fake, y_fake = get_training_dataset(fake_datas)\n",
    "# print(x_real)\n",
    "# print(y_real)\n",
    "# print(x_fake)\n",
    "# print(y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 提取统计特征（均值、方差、最大值、最小值等）\n",
    "def extract_features(X):\n",
    "    features = []\n",
    "    for seq in X:\n",
    "        seq = np.array(seq)\n",
    "        mean_feat = np.mean(seq, axis=0)\n",
    "        std_feat = np.std(seq, axis=0)\n",
    "        max_feat = np.max(seq, axis=0)\n",
    "        min_feat = np.min(seq, axis=0)\n",
    "        first_feat = seq[0]\n",
    "        length = len(seq)\n",
    "        features.append(np.hstack([mean_feat, std_feat, max_feat, min_feat, first_feat, length]))\n",
    "    return np.array(features)\n",
    "\n",
    "def test_xgboost(X_train, Y_train, X_test, Y_test):\n",
    "    X_train_features = extract_features(X_train)\n",
    "    X_test_features = extract_features(X_test)\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train_features, Y_train)\n",
    "    Y_pred = model.predict(X_test_features)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = [torch.tensor(x, dtype=torch.float32) for x in X]\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    X, Y = zip(*batch)\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)  # 填充变长序列\n",
    "    lengths = torch.tensor([len(x) for x in X])  # 记录原始长度\n",
    "    return X_padded, lengths, torch.tensor(Y)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, device):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths.to('cpu'), batch_first=True, enforce_sorted=False).to(self.device)\n",
    "        _, (hidden, _) = self.lstm(packed_x)\n",
    "        return self.fc(hidden[-1])  # 取最终隐藏状态\n",
    "    \n",
    "def test_lstm(X_train, Y_train, X_test, Y_test):\n",
    "    train_dataset = TrafficDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "    test_dataset = TrafficDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(device)\n",
    "    model = LSTMClassifier(SEQ_DIM, 128, label_dim, device).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for X, lengths, Y in train_loader:\n",
    "            X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "            Y_pred = model(X, lengths)\n",
    "            loss = criterion(Y_pred, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, lengths, Y in test_loader:\n",
    "                X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "                Y_pred = model(X, lengths)\n",
    "                _, predicted = torch.max(Y_pred, 1)\n",
    "                total += Y.to('cpu').size(0)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "        accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, device):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # 全局池化\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # 交换维度以适应 Conv1d (batch, feature_dim, seq_len)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "def test_cnn(X_train, Y_train, X_test, Y_test):\n",
    "    train_dataset = TrafficDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "    test_dataset = TrafficDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(device)\n",
    "    model = CNNClassifier(SEQ_DIM, label_dim, device).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for X, lengths, Y in train_loader:\n",
    "            X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "            Y_pred = model(X)\n",
    "            loss = criterion(Y_pred, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, lengths, Y in test_loader:\n",
    "                X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "                Y_pred = model(X)\n",
    "                _, predicted = torch.max(Y_pred, 1)\n",
    "                total += Y.to('cpu').size(0)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "        accuracy = correct / total\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaozj/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:40:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/xiaozj/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:40:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/xiaozj/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:40:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on real data of test_xgboost: 0.930715935334873\n",
      "Accuracy on fake data of test_xgboost: 0.2702078521939954\n",
      "Accuracy on mixed data of test_xgboost: 0.9284064665127021\n",
      "Accuracy on real data of test_cnn: 0.8579676674364896\n",
      "Accuracy on fake data of test_cnn: 0.3787528868360277\n",
      "Accuracy on mixed data of test_cnn: 0.848729792147806\n",
      "Accuracy on real data of test_lstm: 0.8845265588914549\n",
      "Accuracy on fake data of test_lstm: 0.3579676674364896\n",
      "Accuracy on mixed data of test_lstm: 0.8822170900692841\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_real, y_real, test_size=0.2, random_state=42)\n",
    "X_train_fake, X_test_fake, Y_train_fake, Y_test_fake = train_test_split(x_fake, y_fake, test_size=0.2, random_state=42)\n",
    "\n",
    "methods = [test_xgboost, test_cnn, test_lstm]\n",
    "\n",
    "for func in methods:\n",
    "    accuracy_real = func(X_train, Y_train, X_test, Y_test)\n",
    "    accuracy_fake = func(X_train_fake, Y_train_fake, X_test, Y_test)\n",
    "    accuracy_mix = func(X_train_fake + X_train, Y_train_fake + Y_train, X_test, Y_test)\n",
    "    print(f'Accuracy on real data of {func.__name__}:', accuracy_real)\n",
    "    print(f'Accuracy on fake data of {func.__name__}:', accuracy_fake)\n",
    "    print(f'Accuracy on mixed data of {func.__name__}:', accuracy_mix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
