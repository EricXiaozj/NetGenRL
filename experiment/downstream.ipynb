{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "from seqCGAN.generator import Generator  # 假设你有一个定义好的 Discriminator 类\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import struct\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_LEN = 114\n",
    "# NPRINT_REAL_WIDTH = 50*8\n",
    "NPRINT_REAL_WIDTH = 22*8\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'aim': 2, 'email': 3, 'voipbuster': 4, 'hangouts': 5, 'youtube': 6, 'sftp': 7, 'icq': 8,  'ftps': 9, 'vimeo': 10, 'spotify': 11, 'netflix': 12, 'bittorrent': 13}\n",
    "LABEL_DICT = {'facebook': 0, 'skype': 1}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'hangouts': 4, 'youtube': 5, 'ftps': 6, 'vimeo': 7, 'spotify': 8, 'netflix': 9, 'bittorrent': 10}\n",
    "# LABEL_DICT = {'facebook': 0, 'skype': 1, 'email': 2, 'voipbuster': 3, 'youtube': 4, 'ftps': 5, 'vimeo': 6, 'spotify': 7, 'netflix': 8, 'bittorrent': 9}\n",
    "# LABEL_DICT = {'email': 0, 'youtube': 1, 'ftps': 2, 'vimeo': 3, 'spotify': 4, 'netflix': 5, 'bittorrent': 6}\n",
    "\n",
    "SEQ_DIM = 2\n",
    "MAX_PKT_LEN = 3001\n",
    "MAX_TIME = 10000\n",
    "MAX_PORT = 65535\n",
    "MAX_SEQ_LEN = 16\n",
    "WORD_VEC_SIZE = 8\n",
    "# SEQ_DIM = WORD_VEC_SIZE * 2 + 1\n",
    "\n",
    "label_dim = len(LABEL_DICT) \n",
    "image_dim = (1, NPRINT_REAL_WIDTH, NPRINT_REAL_WIDTH)  # 生成单通道图像\n",
    "noise_dim = 100  # 噪声维度\n",
    "batch_size = 128\n",
    "source_name = '../data/vpn_data_small.json'\n",
    "bins_name = '../bins/bins_small_new.json'\n",
    "model_name = '../save_seq/generator_1200.pth'\n",
    "\n",
    "with open('../wordvec/word_vec_small.json', 'r') as f:\n",
    "    wv_dict = json.load(f)\n",
    "    \n",
    "wv = {}\n",
    "for key, metrics in wv_dict.items():\n",
    "    wv[key] = torch.tensor(metrics, dtype=torch.float32)\n",
    "\n",
    "x_list = [wv_tensor.size(0) for wv_tensor in wv.values()]\n",
    "# x_list = [MAX_TIME, MAX_PKT_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_data = {}\n",
    "with open(bins_name, 'r') as f_bin:\n",
    "    bins_data = json.load(f_bin)\n",
    "    \n",
    "port_intervals = bins_data['port']['intervals']\n",
    "pkt_len_intervals = bins_data['packet_len']['intervals']\n",
    "time_intervals = bins_data['time']['intervals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1171225/3048651150.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_name)  # 加载保存的权重字典\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (emb): ModuleList(\n",
       "    (0): Embedding(41, 128)\n",
       "    (1): Embedding(369, 128)\n",
       "  )\n",
       "  (condition_fix): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (length_fc): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (combine_fc): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=320, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(512, 512, num_layers=4, batch_first=True)\n",
       "  (output_layer): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=576, out_features=41, bias=True)\n",
       "      (1): Identity()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=576, out_features=369, bias=True)\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generator = Generator(label_dim, noise_dim, SEQ_DIM, MAX_SEQ_LEN, 'cpu')\n",
    "generator = Generator(label_dim,SEQ_DIM,MAX_SEQ_LEN,x_list,'cpu')\n",
    "\n",
    "# 加载模型权重\n",
    "checkpoint = torch.load(model_name)  # 加载保存的权重字典\n",
    "generator.load_state_dict(checkpoint)  # 将权重字典加载到模型中\n",
    "\n",
    "# 切换到评估模式\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datas, label_str):\n",
    "        \"\"\"\n",
    "        :param sequences: 一个包含真实序列的列表，每个序列是一个 ndarray 或 list\n",
    "        \"\"\"\n",
    "        self.datas = datas\n",
    "        self.lengths = [len(seq) for seq in datas]  # 提取序列长度\n",
    "        label_id = LABEL_DICT[label_str]\n",
    "        self.label_one_hot = torch.zeros(label_dim).to('cpu') \n",
    "        self.label_one_hot[label_id] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lengths[idx],self.label_one_hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_data(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        json_data = json.load(f)['data']\n",
    "    data_dic = {}\n",
    "    \n",
    "    for item in json_data:\n",
    "        label_str = item['labels'][0]\n",
    "        if label_str not in data_dic:\n",
    "            data_dic[label_str] = []\n",
    "\n",
    "        packets_len = item['meta']['packets']\n",
    "        im = bytes.fromhex(item['nprint'])\n",
    "\n",
    "        line = im[0:TOTAL_LEN]\n",
    "        tcp_dport = line[32:34]\n",
    "        udp_dport = line[92:94]\n",
    "\n",
    "        dport = bytearray(a | b for a, b in zip(tcp_dport, udp_dport))\n",
    "        dport = int.from_bytes(dport, 'big')\n",
    "    \n",
    "        data_item = []\n",
    "        for i in range(min(16,packets_len)):\n",
    "            line = im[i*TOTAL_LEN:i*TOTAL_LEN+TOTAL_LEN]\n",
    "            # time_h, time_l, pl = struct.unpack(\"IIh\", line[:10])\n",
    "            # time_l //= 1e4\n",
    "            # time = time_h + time_l/100\n",
    "            time_h,time_l, pkt_len = struct.unpack(\"IIh\", line[:10])\n",
    "            time_l //= 1e4\n",
    "            time = time_h + time_l/100\n",
    "            time = time_h % 1000 + time_l / 10\n",
    "            \n",
    "            data_item.append([time, pkt_len])\n",
    "            \n",
    "        data_dic[label_str].append(data_item)\n",
    "        \n",
    "    return data_dic\n",
    "\n",
    "def get_fake_data(real_data, label_str):\n",
    "    dataset = SequenceDataset(real_data,label_str)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "    generated_sequences = []\n",
    "    with torch.no_grad():\n",
    "        for lengths, labels in dataloader:\n",
    "            lengths = lengths.to(torch.device(\"cpu\"))  # 确保在同一个设备上\n",
    "            labels = labels.to(torch.device(\"cpu\"))\n",
    "            # print(lengths.shape)\n",
    "            # print(labels.shape)\n",
    "            batch_size = lengths.size(0)\n",
    "            # 生成随机噪声向量\n",
    "            # noise = torch.randn(len(lengths), noise_dim)\n",
    "            # 输入生成器生成数据\n",
    "            fake_data = generator.sample(batch_size,labels,lengths)\n",
    "            # 将生成结果按序列长度截断\n",
    "            for i, length in enumerate(lengths):\n",
    "                generated_sequences.append(fake_data[i, :length].cpu().tolist())\n",
    "       \n",
    "    final_seqs = []         \n",
    "    for seq in generated_sequences:\n",
    "        f_seq = []\n",
    "        for i in range(len(seq)):\n",
    "            [time,pkt_len] = seq[i]       \n",
    "            # time_id = math.floor((time + 1)/2*len(time_intervals[i]))\n",
    "            # pkt_len_id = math.floor((pkt_len + 1)/2*len(pkt_len_intervals[i]))\n",
    "            # dport_id = math.floor((dport + 1)/2*len(port_intervals))\n",
    "        \n",
    "            time = round(random.uniform(time_intervals[time][0], time_intervals[time][1]),2)\n",
    "            pkt_len = round(random.uniform(pkt_len_intervals[pkt_len][0], pkt_len_intervals[pkt_len][1]))\n",
    "            # dport = round(random.uniform(port_intervals[dport_id][0],port_intervals[dport_id][1]))\n",
    "            time = time\n",
    "            pkt_len = pkt_len\n",
    "            f_seq.append([time,pkt_len])\n",
    "        final_seqs.append(f_seq)\n",
    "    return final_seqs\n",
    "\n",
    "def pad(sequence, target_length, pad_value=np.nan):\n",
    "    seq_len = len(sequence)\n",
    "    if seq_len < target_length:\n",
    "        padding = [[pad_value] * len(sequence[0])] * (target_length - seq_len)\n",
    "        return sequence + padding  # 填充\n",
    "    return sequence\n",
    "\n",
    "def get_training_dataset(data_dic):\n",
    "    x = []\n",
    "    y = []\n",
    "    label_count = 0\n",
    "    for label, data in data_dic.items():\n",
    "        x.extend(data)  # 添加所有序列\n",
    "        y.extend([label_count] * len(data))\n",
    "        label_count += 1\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_datas = get_real_data(source_name)\n",
    "\n",
    "fake_datas = {}\n",
    "for label, data in real_datas.items():\n",
    "    fake_data = get_fake_data(data,label)\n",
    "    fake_datas[label] = fake_data\n",
    "    \n",
    "x_real, y_real = get_training_dataset(real_datas)\n",
    "x_fake, y_fake = get_training_dataset(fake_datas)\n",
    "# print(x_real)\n",
    "# print(y_real)\n",
    "# print(x_fake)\n",
    "# print(y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 提取统计特征（均值、方差、最大值、最小值等）\n",
    "def extract_features(X):\n",
    "    features = []\n",
    "    for seq in X:\n",
    "        seq = np.array(seq)\n",
    "        mean_feat = np.mean(seq, axis=0)\n",
    "        std_feat = np.std(seq, axis=0)\n",
    "        max_feat = np.max(seq, axis=0)\n",
    "        min_feat = np.min(seq, axis=0)\n",
    "        first_feat = seq[0]\n",
    "        length = len(seq)\n",
    "        features.append(np.hstack([mean_feat, std_feat, max_feat, min_feat, first_feat, length]))\n",
    "    return np.array(features)\n",
    "\n",
    "def test_xgboost(X_train, Y_train, X_test, Y_test):\n",
    "    X_train_features = extract_features(X_train)\n",
    "    X_test_features = extract_features(X_test)\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train_features, Y_train)\n",
    "    Y_pred = model.predict(X_test_features)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = [torch.tensor(x, dtype=torch.float32) for x in X]\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    X, Y = zip(*batch)\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)  # 填充变长序列\n",
    "    lengths = torch.tensor([len(x) for x in X])  # 记录原始长度\n",
    "    return X_padded, lengths, torch.tensor(Y)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, device):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths.to('cpu'), batch_first=True, enforce_sorted=False).to(self.device)\n",
    "        _, (hidden, _) = self.lstm(packed_x)\n",
    "        return self.fc(hidden[-1])  # 取最终隐藏状态\n",
    "    \n",
    "def test_lstm(X_train, Y_train, X_test, Y_test):\n",
    "    train_dataset = TrafficDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "    test_dataset = TrafficDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(device)\n",
    "    model = LSTMClassifier(SEQ_DIM, 128, label_dim, device).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for X, lengths, Y in train_loader:\n",
    "            X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "            Y_pred = model(X, lengths)\n",
    "            loss = criterion(Y_pred, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, lengths, Y in test_loader:\n",
    "                X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "                Y_pred = model(X, lengths)\n",
    "                _, predicted = torch.max(Y_pred, 1)\n",
    "                total += Y.to('cpu').size(0)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "        accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, device):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # 全局池化\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # 交换维度以适应 Conv1d (batch, feature_dim, seq_len)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "def test_cnn(X_train, Y_train, X_test, Y_test):\n",
    "    train_dataset = TrafficDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "    test_dataset = TrafficDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(device)\n",
    "    model = CNNClassifier(SEQ_DIM, label_dim, device).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for X, lengths, Y in train_loader:\n",
    "            X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "            Y_pred = model(X)\n",
    "            loss = criterion(Y_pred, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, lengths, Y in test_loader:\n",
    "                X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
    "                Y_pred = model(X)\n",
    "                _, predicted = torch.max(Y_pred, 1)\n",
    "                total += Y.to('cpu').size(0)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "        accuracy = correct / total\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaozj/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [10:47:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/xiaozj/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [10:47:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/xiaozj/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [10:47:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on real data of test_xgboost: 0.930715935334873\n",
      "Accuracy on fake data of test_xgboost: 0.7124711316397229\n",
      "Accuracy on mixed data of test_xgboost: 0.9318706697459584\n",
      "Accuracy on real data of test_cnn: 0.8833718244803695\n",
      "Accuracy on fake data of test_cnn: 0.8060046189376443\n",
      "Accuracy on mixed data of test_cnn: 0.8625866050808314\n",
      "Accuracy on real data of test_lstm: 0.8960739030023095\n",
      "Accuracy on fake data of test_lstm: 0.8279445727482679\n",
      "Accuracy on mixed data of test_lstm: 0.894919168591224\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_real, y_real, test_size=0.2, random_state=42)\n",
    "X_train_fake, X_test_fake, Y_train_fake, Y_test_fake = train_test_split(x_fake, y_fake, test_size=0.2, random_state=42)\n",
    "\n",
    "methods = [test_xgboost, test_cnn, test_lstm]\n",
    "\n",
    "for func in methods:\n",
    "    accuracy_real = func(X_train, Y_train, X_test, Y_test)\n",
    "    accuracy_fake = func(X_train_fake, Y_train_fake, X_test, Y_test)\n",
    "    accuracy_mix = func(X_train_fake + X_train, Y_train_fake + Y_train, X_test, Y_test)\n",
    "    print(f'Accuracy on real data of {func.__name__}:', accuracy_real)\n",
    "    print(f'Accuracy on fake data of {func.__name__}:', accuracy_fake)\n",
    "    print(f'Accuracy on mixed data of {func.__name__}:', accuracy_mix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
